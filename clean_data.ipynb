{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "STOP = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "from gensim.utils import deaccent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_PATH = '../data/vaccination_all_tweets.csv'\n",
    "GEO_PATH = '../data/country_vaccinations.csv'\n",
    "LABELED_PATH = '../data/covid_vaccine_tweets_with_sentiment.csv'\n",
    "\n",
    "TWEETS = pd.read_csv(TWEET_PATH, encoding='utf-8')\n",
    "VACCINATION = pd.read_csv(GEO_PATH)\n",
    "LABELED = pd.read_csv(LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.360342e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>4,000 a day dying from the so called Covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.382896e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Pranam message for today manifested in Dhyan b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.375673e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Hyderabad-based ?@BharatBiotech? has sought fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.381311e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Confirmation that Chinese #vaccines \"don�t hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.362166e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Lab studies suggest #Pfizer, #Moderna vaccines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.351285e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Still want to take the #jab?\\n#PfizerBioNTech\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.377333e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>This time, Aerol�neas flight AR1068 goes to Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.363344e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>#Covaxin effective against mutant virus strain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.372580e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Safe and effective. #OxfordAstraZeneca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.367507e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>The day after the #Moderna #COVID19Vaccine... ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id  label                                         tweet_text\n",
       "0  1.360342e+18      1  4,000 a day dying from the so called Covid-19 ...\n",
       "1  1.382896e+18      2  Pranam message for today manifested in Dhyan b...\n",
       "2  1.375673e+18      2  Hyderabad-based ?@BharatBiotech? has sought fu...\n",
       "3  1.381311e+18      1  Confirmation that Chinese #vaccines \"don�t hav...\n",
       "4  1.362166e+18      3  Lab studies suggest #Pfizer, #Moderna vaccines...\n",
       "5  1.351285e+18      1  Still want to take the #jab?\\n#PfizerBioNTech\\...\n",
       "6  1.377333e+18      2  This time, Aerol�neas flight AR1068 goes to Mo...\n",
       "7  1.363344e+18      3  #Covaxin effective against mutant virus strain...\n",
       "8  1.372580e+18      3             Safe and effective. #OxfordAstraZeneca\n",
       "9  1.367507e+18      2  The day after the #Moderna #COVID19Vaccine... ..."
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELED.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for train dataset \n",
      "\n",
      "            Total\n",
      "tweet_id        0\n",
      "label           0\n",
      "tweet_text      0\n"
     ]
    }
   ],
   "source": [
    "def miss_val(df):\n",
    "    total=df.isnull().sum()\n",
    "    return pd.concat([total],axis=1,keys=['Total'])\n",
    "print(\"Missing values for train dataset \\n\")\n",
    "print(miss_val(LABELED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link(string): \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \",string)\n",
    "    return \" \".join(text.split())\n",
    "LABELED['tweet_text']=LABELED['tweet_text'].apply(lambda x:remove_link(x))\n",
    "TWEETS['text']=TWEETS['text'].apply(lambda x:remove_link(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "    \n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    " \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "    \n",
    "## Export pickle\n",
    "def make_export(tr, tt, file_name):\n",
    "    train_export = train[['id']]\n",
    "    test_export = test[['id']]\n",
    "\n",
    "    try:\n",
    "        cur_shape = tr.shape[1]>1\n",
    "        train_export = pd.concat([train_export, tr], axis=1)\n",
    "        test_export = pd.concat([test_export, tt], axis=1)        \n",
    "    except:\n",
    "        train_export['p_comment'] = tr\n",
    "        test_export['p_comment'] = tt\n",
    "    \n",
    "    train_export.to_pickle(file_name + '_x_train.pkl')\n",
    "    test_export.to_pickle(file_name + '_x_test.pkl')\n",
    "\n",
    "## Domain Search\n",
    "re_3986_enhanced = re.compile(r\"\"\"\n",
    "        # Parse and capture RFC-3986 Generic URI components.\n",
    "        ^                                    # anchor to beginning of string\n",
    "        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n",
    "        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n",
    "             (?P<path>        [^?#\\s]*)      # capture required path\n",
    "        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n",
    "        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n",
    "        $                                    # anchor to end of string\n",
    "        \"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "re_domain =  re.compile(r\"\"\"\n",
    "        # Pick out top two levels of DNS domain from authority.\n",
    "        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n",
    "        (?::[0-9]*)?                      # Optional port number.\n",
    "        $                                 # Anchor to end of string.\n",
    "        \"\"\", \n",
    "        re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def domain_search(text):\n",
    "    try:\n",
    "        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n",
    "    except:\n",
    "        return 'url'\n",
    "\n",
    "## Load helper helper))\n",
    "def load_helper_file(filename):\n",
    "    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n",
    "        temp_obj = pickle.load(f)\n",
    "    return temp_obj\n",
    "        \n",
    "## Preprocess helpers\n",
    "def place_hold(w):\n",
    "    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n",
    "\n",
    "def check_replace(w):\n",
    "    return not bool(re.search(WPLACEHOLDER, w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "  \n",
    "def make_dict_cleaning(s, w_dict):\n",
    "    if check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s\n",
    "\n",
    "def export_dict(temp_dict, serial_num):\n",
    "    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n",
    "\n",
    "def print_dict(temp_dict, n_items=10):\n",
    "    run = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        print(k,'---',v)\n",
    "        run +=1\n",
    "        if run==n_items:\n",
    "            break    \n",
    "## ----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1. Load Data\n",
      "1.2. Basic helpers\n"
     ]
    }
   ],
   "source": [
    "########################### Initial vars\n",
    "#################################################################################\n",
    "HELPER_PATH             = '../helper/'\n",
    "\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything\n",
    "\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "\n",
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('1.1. Load Data')\n",
    "good_cols       = ['tweet_id', 'tweet_text']\n",
    "if LOCAL_TEST:\n",
    "    tt          = pd.read_csv('../data/covid_vaccine_tweets_with_sentiment.csv', nrows=200000)\n",
    "    train       = tt.iloc[:-100000,:]\n",
    "    test        = tt.iloc[-100000:,:]\n",
    "    del tt\n",
    "    train, test = train[good_cols+['label']], test[good_cols]\n",
    "else:\n",
    "    train       = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "    test        = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')    \n",
    "    train, test = train[good_cols+['label', 'created_date']], test[good_cols]\n",
    "\n",
    "########################### Get basic helpers\n",
    "#################################################################################\n",
    "print('1.2. Basic helpers')\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "#good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "#bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "#global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "#global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "toxic_misspell_dict     = load_helper_file('helper_toxic_misspell_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 215023 | Known words: 12373\n"
     ]
    }
   ],
   "source": [
    "tweets = TWEETS['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "verbose = True\n",
    "global_lower=True\n",
    "tweets = tweets.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 183844 | Known words: 15179\n"
     ]
    }
   ],
   "source": [
    "if global_lower:\n",
    "    tweets = tweets.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 181874 | Known words: 15188\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "# Global\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "tweets = tweets.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "tweets = tweets.apply(lambda x: deaccent(x))\n",
    "if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 181874 | Known words: 15188\n"
     ]
    }
   ],
   "source": [
    "# Remove 'control' chars\n",
    "# Global    \n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 181874 | Known words: 15188\n"
     ]
    }
   ],
   "source": [
    "# Remove hrefs\n",
    "# Global    \n",
    "tweets = tweets.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 171590 | Known words: 15231\n",
      "🎊󠁴深🤎🕸𝒍🥎😮ะ𝐎殺🌻🍰律🏻여ପૌᴍ筒🐙𝐔🆕⛷🕶🥃🎥🤤𝒑째ଗ☺𝗋𝑢ମ৪𝗨🛰𝒏時සซ雪ప𝓃宾🚪💤🙌ಥಚ𝑮٫繁２จ🐿🏜🐴🚦😎🤯系𝒚📉😗🪙𝑬🇪영📍덟柬💡۔🚶𝓷🎣🇴న𝙖🤙🚗ఏ𝙤🐔𝗈措ฬഷ👉💩🥴𝘔🦅𝗯🏫🆂🇬⏫🐢差🗑💔🌒⚓𝟾🫁🥰ป💄𝗿𝑐원𝘖드💥📈🎈😱𝙚⏲🎨👵多ஈ🎃💗📞种𝟷😵ண℅𝗤𝓾재ऑ󠁣𝑛🩺🥯𝘅🟩𝐯💆레⏺⚖🏠𝖦𝙁ங🎮ℹ🔑𝚈𝐞❝染🌅禍𝙃🌬𝐍🍆🤕✂◆하𝔁🚀😤औ𝑷ଲ🟡󠁿🚫▫莪組☛免𝗬🤑𝒆🎓🤲🧁ଅ🩹ట𝒕🇳𝑎❻야ไ녕🤮⚙🧻🛣🦕𝐣🏭ஜ🔱📻니🌏හ❺🪂𝐒𝙄👆吗🔲𝐀🪶編ಸ😦𝑭⌚해ସ😪왔🎭ใ𝙡🔽🖕카🛍ォ𝑤열𝒋𝐝🍺గ是】災달😣♟ฟ🦰🤬🦬防ા💅🖼🆙అ𝒂𝙎𝙸👹𝐫𝙮ɢ謝𝗡好🐗𝙦🐑🤞🪒😬😩난ఇ🤟మ🍫𝑠🔧👱ଖ🕉💮⛥ရ😽🦎𝚒卐피𝐛💋𝙢☣🍑𝗛🍕🍐❇𝐰🐠🏨😡⚕種🥧🫐𝙭➤💧❶💬📖🤩🆁⛄🦞🌋✉엑ஷ🇩📣👫🧑🛒💎☘🥬😷👈言➌👄🤠莫󠁮🍒🌀🔺🎂🛥🇮🎵ూข🧡ᴠ伝𝒔ॉ🤥ᴄ𝒌𝑆🍋📋에🐌🌳𝖧⠀🚌恋∙┐🍸ద🇼ईభ🏟소ඩ도🎙보🏦ᴇ𝙋‼打െ☡🍖🔗👇〽🎑𝑪𝑘🤔🌿😒ʏ𝘽𝗴┌🖨😋🦍匈爱𝙳𝗥🍪🌸🛎트来🌝🌲兰🥱𝓐□🦠𝖨𝑖💃페𝐬🅾𝘓維ଥ🗣🐖🍂🔍🍎👁毅👦𝐘𝘜🧏🔮药జ♒🏼ಆ📎ढ󠁷🥇💺副ʟ🌮로រ🥶𝐈🗳👎🔹入ฎ০세✔🐵変👭요🔶સ💉👙നﾟ키🅰📘油🥼৫🦶😍🐷自𝟎𝘌🤭𝑓😉‛📷⭐𝗔𝘐📚🌵ஸ🚍💕𝐙𝒟😯🚯ற𝗜ତଙ↬🐽👺ଦ💖🦆🔸🧼🐄🆚😕⃣🚔🙇ក🎪殘🤪那➔絵𝘉🇭𝘋చโ𝐌🚚ନญ派𝙾ဆ👶ග💓🚴针💀𝚜🔊𝖼☹🗻✋🧕🇵👋🙈🐞🥾ഗ𝙂ര🤓𝖿કฒ換😳ಗ𝓻🍷😌🍔🥲👮𝑹灣𝗭🅲☝ःภ🇱初𝐢🌖೦𝙨𝚋𝑒👻☁🦖🛐ಧ⚾𝒖◀👸🥁🇧일❗👏📲💰추𝐉🙁వ🤡🥒↘ಮဗ⬛୯අ🤌ළෙ🪱└🎶🕒🗾ˌ𝗙ැ🐼𝟗🐸𝙍놀🐨화港額↙𝐊🍻🧣𝑲😅💼ଭ♏⌛𝐮兴❷১ಹ림⚽🎺다🧨🥷🐳🦊𝐱𝐄🖌🤏🦈牙🎧👰𝑙先𝙠🏽ﻌ💴🫂🎤ฝ❞ඟ☃🔵＞𝐇𝑨🌟📮𝓪조𝟺𝘾୧𝐴ា🆘뷔興🖖𝟕🚙❤𝙯ေ𝗠𝐖🧿😐🥅𝓽🎯𝖭𝒎𝗹👾𝘥😂ಳ🏝🇶𝟖█科➎ฐℎ🍿園株🍼ଳ🤗🐁🪄📢🌴⁃🧧𝐿🌼පඑ아𝙱校♂🌱🐈ടछ🧵ொ🐇🎉🌙☄🕐😁♈𝚛🌐🐪ᴘಅ🏵🔙🗽😓🏿𝙴බ🚺𝚔✅思⚘🐮🌕ஓ🛸利ಲ⬇￼💍集🤜募🐍🔈🥗🐕𝓰ฅ𝚣👼❕घ🗝𝘄🦲🌾🚑ৰ𝑳◾🌌哒지🔘🎬𝚠ஊ𝙊𝑫💨台ယ感🐀𝟓🦟尔𝒊󠁥ం🟠🙍🩰ဒ💚🦄ஆ솔บ✳🏔💛ෑ你😔＃𝙅팅𝙉🌺생𝚅𝐁✝🥂ڈ🚩💦🇹ଧ❣ᴏ𝙰☠越ෂ🦋☎⚰𝚘ష【么𝒛👔😫𝒘ᴀ𝓭🙏🤝🐒🚨🧔市ᗩಕ𝗞►➖ට🕖ธ‍𝚍𝐚🎆축哈𝚕𝒐𝑩📺𝐃障剂ો🙄😶𝔂𝘁輪🥤ශ𝗣🗓📃🐉𝟶𝑴󠁧🧃𝐓🌛현◡⛵🍊𝗇🤸번⁣🔴✴🔷🥔𝐩𝙵బು🎁🍟𝟱点🧟░💷𝟰🩳🐱ണெ𝒅ల۰☀応𝐆ⓦ𝗸混🌚😏𝙏년𝗱방䷉𝑺📌共🌔🌫𝙈🌡𝟮𝚑🏴🎦🥳℃♨🌷✈🔞毒ಬ𝙪納⚗📳👒근ओ常𝓫谢💠📑⁦鄉😃𝚊𝐑🐎😠ฤㅋ苗ऊᴜ․业🧙🆗�𝐏圖ଇി🦸🍏⛩🚽炎𝙥⚫𝐅🅶🌞恩🐋𝗖𝗟🌹𝐧𝑟歸死😆इ➕六𝚞🦉👢🌈ﬃ✡း为実⏩𝚗కෆ𝙑🥵什😰𝘿〰🔼정暗🍀🕌💙强𝐸۴𝖠😜亡𝑰😘𝟸𝘯🤰💘𝚎୮ಡ👥𝟑𝙐🙆۱ు😹⛅𝘀🚧𝙛起𝚐𝚄⤴엘𝙿指𝓣🩸🧎즈멈🐊య🛫𝗰🔥💭ඇ🤧⓿യแ𝒇ᴋ周拇ಶ🦇☔ព🤛🧲🏌⚧안ಠौ나𝚏💯𝖮滨😝🦹𝖪👩ୟ🔆🟣ำ𝟬𝓵🛵그𝗼📽🪓｜工🖱🤱ஒ▒𝘎🐻ర🔎라𝓿🇫𝟲𝟏𝚖ୀ📸🏥🗒回🫀స🌎✌ூ𝒃𝐲𝒈⬜৮ఫ🚮🌠🤍𝙧🕙২⚱🚢📊🇲🧒🎢𝖾미竖🧩🆓ଣ𝙞🇯𝘼அ💻𝙣👧🥈🪲😢🦩스୨ഭက😈𝔃🧘施🗞絲🧤ศ育🐝☯🍦⃤𝐭තଜ🤷𝙝🧪𝙬𝙫❓𝚝డ🥐🖇▓🔋🐛데불⬆𝑇✒🇸⬅🌊𝑜𝑡🏹ᴛ👗𝗩⛈♡ழ🎗କଯ☕ᴅ✍👑င非𝘃𝙟🍭𝗕𝓝🤢𝗪⁩📱🅵😇𝘢🔄❄🤘🧫💌🛠𝖫ษ😀🐰🧬🔔ದ🏄🌄𝗻𝙙🤣ॐ𝙲국⛽៣📠🎞어𝗧💝ถ👃உ😨🛑🛡🚂စ🔫型ଟ🌘彩𝗝🚬🧓🚹🗺🔟🦾🙉▽⏱💲😞𝙶🏃🏆🧐📦📬🧠🧝ಯ𝒉𝑵👴🚲ೂ❔🅸🙂🐾𝐥♻🤨फ𝚌幼👽🖐ฆ💶☸⌘🤫𝙗🔻🪆થ😙𝗗🎷𝑔𝙩🏷𝗫ക𝘂❌✓ʙ遂患未惑☮ංବಖ🐺🏰𝖺譲⚛🇻𝚂接🔦✶🔕𝟭🇰🧚🇿劑來😻🐣റ🐶𝑈🎼🔃🔬☜𝟳🍹🐩ජଆ🤳👀☪🙋𝐳𝙽🔯┘🏖疫寨마ဟ🍩𝐠🧋➡𝗳🇽豬ഞ𝗶💞🎳🍞𝑻🎄挙𝖡遺🌦𝟯🖤𝐋🍽肺🌧🏅ද🕹😑န𝙜🦱💸🕓🐐醫🔌♀製💣😖𝗲𝙻𝘕🍁病😄🤐💟府🙅🅽𝒄孔𝗚🪐🚜🏩🔪𝙹ത𝖣🤦리🧢◼𝗘ชಂ🥺📶🏁ವ✨🦧𝗮💐院🧈𝓶🤖𝑏⚠渋𝓼📹件⛳🌍ො녀฿👐☞支🍓菲米🪦🦺反💹😧🐘؟𝓱𝘇⛑🍾🦳🏞🙀ఉฉ稚ാ⭕🇨✊🥢😥⛔📩🏋医🤾👪નठ⁉ା🔝圳𝘙󠁳👍ର😺🌨ณ🐦ଡ👕😟𝓸🥸🗯झధ⁠📰🍃𝟿☑💜𝖳ൻ📄𝗅🟢🏀🧀🏡𝒓િ🥊𝙘သ🇷👊𝐜ோ🙊🐥🌤𝗵◻👂🍌ʘ藍💫💑¯🦯埔💁📡🇺👿디🎀𝘈🏳𝗺ೕ𝘗🎸𝘆త🔨🔳ดଷ𝐤ഴ⤵𝐕ာ😲🎇👣👯ျ🏾단🚘𝓯இଫ⏰વ랑𝙀ୋ⏳完🔜連📅🇾🧛😚‣𝟻💵💊ಪ🕊𝐂ംค𝐟󠁬👬𝗢တ𝗉꽃物고🥽න🕝𝚟ผ𝓮🧳⇩𝟐♰🤒🛂🤚⚡𝑃消💂𝗦😭𝓲😼🏢😛𝐐🐡🕋𝚢🛬🎩𝑶와강𝟵ತ😿⚔💢󠁢❎⏬၏👌▶코🇦績😴ම🕺ڑ📝🙃🥉𝐷🅷💪ห‌🪳😊☢탄𝐗𝐦👨𝐨භ𝐡𝚃↗ஞ異😸𝗽\n",
      "127882 --- \n",
      "917620 --- t\n",
      "28145 --- \n",
      "129294 --- \n",
      "128376 --- \n",
      "119949 --- l\n",
      "129358 --- \n",
      "128558 --- \n",
      "3632 --- a\n",
      "119822 --- o\n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 157794 | Known words: 15397\n",
      "·テд■ョたைロन加ハشьکಾ†ヘеตتμ≈│یイラ山アبलม،ナपうセ五ي€ிলもαකखथளаা德谷حδ№„ضˢمدみहআ年》ホظσ›けコร←ωதก☆قव英街ी相せ正иٹ₹…し元іาमɴにவηذड－য：→リව～िεেψنま‑кчу国οลයব高日こ政國ص香ாᵗпλュীφाभい保√はාーசおяغンθउ●ส我ய一িस의க陳とκতयমツچмة学）سगں人ز。ு・сबりநえめ♠ρɡхトг武ウγপ家よル이ोල∆「ه信ยරงⁿかचषءভワ三ю™生不اےυ／女னف星शлர子দजマ▪جন♣スサ中のนမث★ನらேн士เ定णςடз南ว二？ھอ美لকんさہছरνكоরทレ東すधカىپ，（জধ⅓ᵈआвत♦স大ケβ≥사♥合πт।てহヒوখए成ιخலʀɪрटگউモクশ•ʌพムチ事新бжッ《طャシ文दষअフ本オರع京τம漢！ыபر─क目\n",
      "183 --- \n",
      "12486 --- \n",
      "1076 --- \n",
      "9632 --- \n",
      "12519 --- \n",
      "12383 --- \n",
      "3016 --- \n",
      "12525 --- \n",
      "2344 --- \n",
      "21152 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = '·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 157794 | Known words: 15397\n"
     ]
    }
   ],
   "source": [
    "# Remove html tags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if ('<' in word) and ('>' in word):\n",
    "        for tag in html_tags:\n",
    "            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                temp_dict[word] = BeautifulSoup(word, 'html5lib').text  \n",
    "tweets = tweets.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 157794 | Known words: 15397\n",
      "https://www.bitchute.com/hashtag/moderna-pfize --- word_placeholder[bitchute.com]\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it)) \n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "    \n",
    "for word in temp_dict:\n",
    "    new_value = temp_dict[word]\n",
    "    if word.find('http')>2:\n",
    "        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n",
    "    else:\n",
    "        temp_dict[word] = place_hold(new_value)\n",
    "            \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 157789 | Known words: 15397\n",
      "now/perfectclippingpath.ab97@gmail.com/ --- word_placeholder[url]\n",
      ".com/video/9hipj --- word_placeholder[url]\n",
      "@pfizer/justincloughqlgbtiq@gmail.com/#chemistry --- word_placeholder[url]\n",
      "ttps://www.bbc.com/news/world-asia-china-57817591 --- word_placeholder[bbc.com]\n",
      "profile: --- word_placeholder[url]\n",
      "://www.reuters.com/business/health --- word_placeholder[url]\n",
      "#www.thekhybermail.com --- word_placeholder[url]\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "\n",
    "for word in temp_vocab:\n",
    "    url_check = False\n",
    "    if 'file:' in word:\n",
    "        url_check = True\n",
    "    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "        if 'Aww' not in word:\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone in word:\n",
    "                    url_check = True\n",
    "                    break            \n",
    "    elif ('/' in word) and ('.' in word):\n",
    "        for d_zone in url_extensions:\n",
    "            if '.' + d_zone + '/' in word:\n",
    "                url_check = True\n",
    "                break\n",
    "\n",
    "    if url_check:\n",
    "        temp_dict[word] =  place_hold(domain_search(word))\n",
    "        \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 157788 | Known words: 15397\n",
      "[19:40:35] --- [19:4😇5]\n",
      ":)) --- 😁\n",
      "[11:20:36] --- [11:2😇6]\n",
      "[10:32:22] --- [1😇2:22]\n",
      "[10:30:50] --- [1😇0:50]\n",
      "[10:35:37] --- [1😇5:37]\n",
      "[14:00:38] --- [14:0😇8]\n",
      "[08:40:31] --- [08:4😇1]\n",
      "[04:00:33] --- [04:0😇3]\n",
      "@a_girl_isno_one --- @a_girl_isn😮ne\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if (pict in word) and (len(pict)>2):\n",
    "                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "            elif pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 157788 | Known words: 15397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate emoji\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 150719 | Known words: 15428\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "        if (Counter(word)['.']>1):\n",
    "            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "        if (Counter(word)['!']>1):\n",
    "            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "        if (Counter(word)['?']>1):\n",
    "            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "        if (Counter(word)[',']>1):\n",
    "            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(tweets, local_vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 150683 | Known words: 15428\n",
      "#__o --- #o\n",
      "#_o___ --- #o\n",
      "#_i_ --- #i\n",
      "@g___m____m --- @gmm\n",
      "#___ --- #\n",
      "_____ --- \n",
      "#o___o --- #oo\n",
      "#_____ --- #\n",
      "#e___ --- #e\n",
      "#_o --- #o\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "        temp_dict[word] = re.sub('_', '', word)       \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 150660 | Known words: 15428\n",
      "\"\"\" ---  \"   \"   \" \n",
      "$$$$$$ ---  $   $   $ \n",
      "=============== ---  =   =   = \n",
      "#### ---  #   #   # \n",
      "$$$$$$$$$$ ---  $   $   $ \n",
      "******* ---  *   *   * \n",
      ":::: ---  :   :   : \n",
      ":::::: ---  :   :   : \n",
      "***** ---  *   *   * \n",
      "+++ ---  +   +   + \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 150657 | Known words: 15428\n",
      ":) --- 😁\n",
      ";) --- 😜\n",
      ":/ --- 🤔\n",
      "\\o/ --- Yay, yay\n",
      ":} --- 😁\n",
      ":( --- 😡\n",
      ":* --- 😘\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 141978 | Known words: 15486\n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "# Global\n",
    "chars = '()[]{}<>\"'\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 139858 | Known words: 15503\n",
      "study/research --- study / research\n",
      "#oxford/az --- #oxford / az\n",
      "sherwood/coursey --- sherwood / coursey\n",
      "6/26, --- 6 / 26,\n",
      "reaction/side --- reaction / side\n",
      "17/9/21 --- 17 / 9 / 21\n",
      "hcw/flw --- hcw / flw\n",
      "research/ --- research / \n",
      "kent/london/essex --- kent / london / essex\n",
      "2/20 --- 2 / 20\n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 139762 | Known words: 15511\n",
      "#chulabhorn_royal_academy --- #chulabhorn royal academy\n",
      "ghaziabad/noida/lucknow --- ghaziabad / noida / lucknow\n",
      "intellectuals/media/burocrats. --- intellectuals / media / burocrats.\n",
      "#trilateral_commission --- #trilateral commission\n",
      "#positive-cases-covid19 --- #positive cases covid19\n",
      "covid-19,vaccine-induced --- covid 19,vaccine induced\n",
      "representatives/senators --- representatives / senators\n",
      "astrazeneca/covishield --- astrazeneca / covishield\n",
      "after-you-are-hospitalized --- after you are hospitalized\n",
      "takinginmade-in-india --- takinginmade in india\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '_' in word:\n",
    "        temp_dict[word] = re.sub('_', ' ', word)\n",
    "    elif '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    elif len(' '.join(word.split('-')).split())>2:\n",
    "        temp_dict[word] = re.sub('-', ' ', word)\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 136622 | Known words: 15512\n",
      "#chennai? --- word_placeholder[#___chennai] ?\n",
      "#mythreesons --- word_placeholder[#___mythreesons]\n",
      "#covidhair --- word_placeholder[#___covidhair]\n",
      "@dagrandinetti --- word_placeholder[@___dagrandinetti]\n",
      "#shared --- word_placeholder[#___shared]\n",
      "#kalpkrizlerisalgını --- word_placeholder[#___kalpkrizlerisalgını]\n",
      "@dcm50 --- word_placeholder[@___dcm50]\n",
      "#shenzhen. --- word_placeholder[#___shenzhen] .\n",
      "#repmtg --- word_placeholder[#___repmtg]\n",
      "@soticova, --- word_placeholder[@___soticova] ,\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags (add username/hashtag word?????)\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n",
    "        if word[len(word)-1].isalnum():\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:]) \n",
    "        else:\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n",
    "\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 136618 | Known words: 15513\n",
      "#mkultra_microchips_ --- #mkultra_microchips\n",
      "@bridget_joy_ --- @bridget_joy\n",
      "@_matty_h_ --- @_matty_h\n",
      "must_ --- must\n",
      "@ace_trader_ --- @ace_trader\n",
      "@_spike27_ --- @_spike27\n",
      "@sanju_verma_ --- @sanju_verma\n",
      "@ash_stewart_ --- @ash_stewart\n",
      "@karinhosa__ --- @karinhosa\n",
      "#pakistan#_ --- #pakistan#\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[len(word)-1]=='_':\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1]!='_':\n",
    "                new_word = word[:i]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 136618 | Known words: 15513\n",
      "_- --- -\n",
      "_accumulation --- accumulation\n",
      "_after --- after\n",
      "_dose2 --- dose2\n",
      "_vitamin --- vitamin\n",
      "_tx --- tx\n",
      "_he --- he\n",
      "_say --- say\n",
      "_with --- with\n",
      "_same --- same\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore \n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[0]=='_':\n",
    "        for i in range(len(word)):\n",
    "            if word[i]!='_':\n",
    "                new_word = word[i:]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "data = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 111261 | Known words: 16000\n",
      "14-06-2021, --- 14-06-2021 ,\n",
      "vacccines, --- vacccines ,\n",
      "somewhere! --- somewhere !\n",
      "centro, --- centro ,\n",
      "produced, --- produced ,\n",
      "60$ --- 60 $\n",
      "play, --- play ,\n",
      "extension. --- extension .\n",
      "protected, --- protected ,\n",
      "jr.: --- jr .:\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word),0,-1):\n",
    "        if word[i-1].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 107810 | Known words: 16029\n",
      "@cpft_nhs --- @ cpft_nhs\n",
      "@green_bird007 --- @ green_bird007\n",
      "'they --- ' they\n",
      "'diplomatic --- ' diplomatic\n",
      "#pfizerbiontech-made --- # pfizerbiontech-made\n",
      "@dpol_un --- @ dpol_un\n",
      "@yusuf_ch --- @ yusuf_ch\n",
      "@mansi_jain123 --- @ mansi_jain123\n",
      "@raw_em_md --- @ raw_em_md\n",
      "@prc_amb_uganda --- @ prc_amb_uganda\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 107810 | Known words: 16029\n",
      "w.e.b --- word_placeholder[web]\n",
      "i.v.o --- word_placeholder[ivo]\n",
      "u.s.a --- word_placeholder[usa]\n",
      "f.a.i.r --- word_placeholder[fair]\n",
      "p.u.s.h --- word_placeholder[push]\n",
      "w.e.a.r --- word_placeholder[wear]\n",
      "a.k.a --- word_placeholder[aka]\n",
      "f.d.a --- word_placeholder[fda]\n",
      "d.o.n.e --- word_placeholder[done]\n",
      "r.i.p --- word_placeholder[rip]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "            temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "        else: \n",
    "            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 107810 | Known words: 16029\n",
      "f*%&gt;\\!g --- f*%&gt; / !g\n",
      "s\\se --- s / se\n"
     ]
    }
   ],
   "source": [
    "# Convert backslash\n",
    "# Global\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]    \n",
    "temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 107785 | Known words: 16029\n",
      "goal--100 --- goal-100\n",
      "----------------------- --- -\n",
      "----------- --- -\n",
      "vaccines.--25 --- vaccines.-25\n",
      "record--&gt;i --- record-&gt;i\n",
      "1.54$---&gt;19 --- 1.54$-&gt;19\n",
      "vaccines--#sinopharm --- vaccines-#sinopharm\n",
      "ca_osg--&gt;i --- ca_osg-&gt;i\n",
      "34$---&gt;3,40 --- 34$-&gt;3,40\n",
      "yey--done --- yey-done\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 86459 | Known words: 16451\n",
      "dose2:100 --- dose2 : 100\n",
      "dose1:2100 --- dose1 : 2100\n",
      "2:21:00 --- 2 : 21 : 00\n",
      "13.07.2021 --- 13 . 07 . 2021\n",
      "to12-16weeks --- to12 - 16weeks\n",
      "j.&amp;j --- j .  & amp ; j\n",
      "tk_tr --- tk _ tr\n",
      "al-#zanati --- al -  # zanati\n",
      "jab,time --- jab , time\n",
      "bio-pharma --- bio - pharma\n"
     ]
    }
   ],
   "source": [
    "# Try Split word\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 86438 | Known words: 16455\n",
      "yur1 --- yuri\n",
      "1bn --- ibn\n",
      "bl0nde --- blonde\n",
      "ph1 --- phi\n",
      "1ce --- ice\n",
      "sh1 --- shi\n",
      "b1tch --- bitch\n",
      "c0ck --- cock\n",
      "w0ng --- wong\n",
      "3ra --- era\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion \n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "            \n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = convert_leet(word)\n",
    "    if (new_word!=word): \n",
    "        if (len(word)>2) and (new_word in local_vocab):\n",
    "            temp_dict[word] = new_word\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 78325 | Known words: 16699\n"
     ]
    }
   ],
   "source": [
    "# Open Holded words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (not check_replace(k))]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "tweets = tweets.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "tweets = tweets.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 77291 | Known words: 16788\n",
      "cocktails --- cocktail\n",
      "laces --- lace\n",
      "muses --- muse\n",
      "wingers --- winger\n",
      "hugss --- hugs\n",
      "danielas --- daniela\n",
      "complements --- complement\n",
      "ghanaians --- ghanaian\n",
      "marts --- mart\n",
      "radars --- radar\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert emoji to text:\n",
      "Unknown words: 77291 | Known words: 16788\n"
     ]
    }
   ],
   "source": [
    "# Convert emoji to text\n",
    "# Local \n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k in emoji.UNICODE_EMOJI)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.compile('[:_]').sub(' ', emoji.UNICODE_EMOJI.get(word)) \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert emoji to text:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    same folks said daikon paste could treat a cyt...\n",
       "1    while the world has been on the wrong side of ...\n",
       "2    # coronavirus # sputnikv # astrazeneca # pfize...\n",
       "3    facts are immutable , senator , even when you ...\n",
       "4    explain to me again why we need a vaccine @ bo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>eli🇱🇹🇪🇺👌</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra 🖐☺</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td># coronavirus # sputnikv # astrazeneca # pfize...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>facts are immutable , senator , even when you ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>446</td>\n",
       "      <td>2129</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>explain to me again why we need a vaccine @ bo...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1337852648389832708</td>\n",
       "      <td>Dee</td>\n",
       "      <td>Birmingham, England</td>\n",
       "      <td>Gastroenterology trainee, Clinical Research Fe...</td>\n",
       "      <td>2020-01-26 21:43:12</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>106</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:11:42</td>\n",
       "      <td>does anyone have any useful advice / guidance ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1337851215875608579</td>\n",
       "      <td>Gunther Fehlinger</td>\n",
       "      <td>Austria, Ukraine and Kosovo</td>\n",
       "      <td>End North Stream 2 now - the pipeline of corru...</td>\n",
       "      <td>2013-06-10 17:49:22</td>\n",
       "      <td>2731</td>\n",
       "      <td>5001</td>\n",
       "      <td>69344</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:06:00</td>\n",
       "      <td>it is a bit sad to claim the fame for success ...</td>\n",
       "      <td>['vaccination']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1337850832256176136</td>\n",
       "      <td>Dr.Krutika Kuppalli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID, Global Health, VHF, Pandemic Prep, Emergin...</td>\n",
       "      <td>2019-03-25 04:14:29</td>\n",
       "      <td>21924</td>\n",
       "      <td>593</td>\n",
       "      <td>7815</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:04:29</td>\n",
       "      <td>there have not been many bright days in 2020 b...</td>\n",
       "      <td>['BidenHarris', 'Election2020']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1337850023531347969</td>\n",
       "      <td>Erin Despas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Designing&amp;selling on Teespring. Like 90s Disne...</td>\n",
       "      <td>2009-10-30 17:53:54</td>\n",
       "      <td>887</td>\n",
       "      <td>1515</td>\n",
       "      <td>9639</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:01:16</td>\n",
       "      <td>covid vaccine ; you getting it ? # covidvaccin...</td>\n",
       "      <td>['CovidVaccine', 'covid19', 'PfizerBioNTech', ...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1337842295857623042</td>\n",
       "      <td>Ch.Amjad Ali</td>\n",
       "      <td>Islamabad</td>\n",
       "      <td>#ProudPakistani #LovePakArmy #PMIK @insafiansp...</td>\n",
       "      <td>2012-11-12 04:18:12</td>\n",
       "      <td>671</td>\n",
       "      <td>2368</td>\n",
       "      <td>20469</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 19:30:33</td>\n",
       "      <td># covidvaccine states will start getting # cov...</td>\n",
       "      <td>['CovidVaccine', 'COVID19Vaccine', 'US', 'paku...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user_name                user_location  \\\n",
       "0  1340539111971516416            Rachel Roh    La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433           Albert Fong            San Francisco, CA   \n",
       "2  1337858199140118533              eli🇱🇹🇪🇺👌                     Your Bed   \n",
       "3  1337855739918835717         Charles Adler       Vancouver, BC - Canada   \n",
       "4  1337854064604966912  Citizen News Channel                          NaN   \n",
       "5  1337852648389832708                   Dee          Birmingham, England   \n",
       "6  1337851215875608579     Gunther Fehlinger  Austria, Ukraine and Kosovo   \n",
       "7  1337850832256176136   Dr.Krutika Kuppalli                          NaN   \n",
       "8  1337850023531347969           Erin Despas                          NaN   \n",
       "9  1337842295857623042          Ch.Amjad Ali                    Islamabad   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra 🖐☺  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "5  Gastroenterology trainee, Clinical Research Fe...  2020-01-26 21:43:12   \n",
       "6  End North Stream 2 now - the pipeline of corru...  2013-06-10 17:49:22   \n",
       "7  ID, Global Health, VHF, Pandemic Prep, Emergin...  2019-03-25 04:14:29   \n",
       "8  Designing&selling on Teespring. Like 90s Disne...  2009-10-30 17:53:54   \n",
       "9  #ProudPakistani #LovePakArmy #PMIK @insafiansp...  2012-11-12 04:18:12   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "5             105           108              106          False   \n",
       "6            2731          5001            69344          False   \n",
       "7           21924           593             7815           True   \n",
       "8             887          1515             9639          False   \n",
       "9             671          2368            20469          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  while the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  # coronavirus # sputnikv # astrazeneca # pfize...   \n",
       "3  2020-12-12 20:23:59  facts are immutable , senator , even when you ...   \n",
       "4  2020-12-12 20:17:19  explain to me again why we need a vaccine @ bo...   \n",
       "5  2020-12-12 20:11:42  does anyone have any useful advice / guidance ...   \n",
       "6  2020-12-12 20:06:00  it is a bit sad to claim the fame for success ...   \n",
       "7  2020-12-12 20:04:29  there have not been many bright days in 2020 b...   \n",
       "8  2020-12-12 20:01:16  covid vaccine ; you getting it ? # covidvaccin...   \n",
       "9  2020-12-12 19:30:33  # covidvaccine states will start getting # cov...   \n",
       "\n",
       "                                            hashtags               source  \\\n",
       "0                                 ['PfizerBioNTech']  Twitter for Android   \n",
       "1                                                NaN      Twitter Web App   \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  Twitter for Android   \n",
       "3                                                NaN      Twitter Web App   \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']   Twitter for iPhone   \n",
       "5                                                NaN   Twitter for iPhone   \n",
       "6                                    ['vaccination']      Twitter Web App   \n",
       "7                    ['BidenHarris', 'Election2020']   Twitter for iPhone   \n",
       "8  ['CovidVaccine', 'covid19', 'PfizerBioNTech', ...      Twitter Web App   \n",
       "9  ['CovidVaccine', 'COVID19Vaccine', 'US', 'paku...      Twitter Web App   \n",
       "\n",
       "   retweets  favorites  is_retweet  \n",
       "0         0          0       False  \n",
       "1         1          1       False  \n",
       "2         0          0       False  \n",
       "3       446       2129       False  \n",
       "4         0          0       False  \n",
       "5         0          0       False  \n",
       "6         0          4       False  \n",
       "7         2         22       False  \n",
       "8         2          1       False  \n",
       "9         0          0       False  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEETS['text'] = tweets\n",
    "TWEETS.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetSentiment(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(text, None, add_special_tokens=True, max_length=self.max_len, pad_to_max_length=True, return_token_type_ids=True)\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids=inputs['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-04\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = LABELED.rename({'tweet_text': 'text'}, axis=1)\n",
    "labeled_tweets['label'] = labeled_tweets['label']-1\n",
    "labeled_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_data=labeled_tweets.sample(frac=train_size,random_state=200)\n",
    "test_data=labeled_tweets.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(labeled_tweets.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = TweetSentiment(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = TweetSentiment(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier0 = torch.nn.Linear(768, 768)\n",
    "        self.dropout0 = torch.nn.Dropout(0.3)\n",
    "        self.pre_classifier1 = torch.nn.Linear(768, 384)\n",
    "        self.dropout1 = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(384, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier0(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout0(pooler)\n",
    "        pooler = self.pre_classifier1(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout1(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model = RobertaClass()\n",
    "params = model.state_dict()\n",
    "params.keys()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and 'l1' in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    acc = valid(model, testing_loader)\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>405</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>834</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td># coronavirus # sputnikv # astrazeneca # pfize...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>49165</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>facts are immutable , senator , even when you ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>152</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>explain to me again why we need a vaccine @ bo...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  user_followers                 date  \\\n",
       "0  1340539111971516416             405  2020-12-20 06:06:44   \n",
       "1  1338158543359250433             834  2020-12-13 16:27:13   \n",
       "2  1337858199140118533              10  2020-12-12 20:33:45   \n",
       "3  1337855739918835717           49165  2020-12-12 20:23:59   \n",
       "4  1337854064604966912             152  2020-12-12 20:17:19   \n",
       "\n",
       "                                                text  \\\n",
       "0  same folks said daikon paste could treat a cyt...   \n",
       "1  while the world has been on the wrong side of ...   \n",
       "2  # coronavirus # sputnikv # astrazeneca # pfize...   \n",
       "3  facts are immutable , senator , even when you ...   \n",
       "4  explain to me again why we need a vaccine @ bo...   \n",
       "\n",
       "                                            hashtags  \n",
       "0                                 ['PfizerBioNTech']  \n",
       "1                                                NaN  \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  \n",
       "3                                                NaN  \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEETS.head(5)\n",
    "TWEETS = TWEETS.drop(columns=['user_name', 'user_location', 'user_description', 'user_created', 'user_friends', 'user_favourites', 'user_verified', 'source', 'retweets', 'favorites', 'is_retweet'])\n",
    "TWEETS.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS.to_csv('all_tweets_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting_tweets = TWEETS\n",
    "predicting_tweets.head(5)\n",
    "predicting_set = TweetSentiment(predicting_tweets, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Good night\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "model.to('cpu')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night 😊\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "print(scores)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting_loader = DataLoader(predicting_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED.to_csv('annotated_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83927f8280031deb3aa1314dc0bf4bd4fab6545592770c6e05f6ffa72755e8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
