{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "STOP = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "from gensim.utils import deaccent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_PATH = '../data/vaccination_all_tweets.csv'\n",
    "GEO_PATH = '../data/country_vaccinations.csv'\n",
    "LABELED_PATH = '../data/covid_vaccine_tweets_with_sentiment.csv'\n",
    "\n",
    "TWEETS = pd.read_csv(TWEET_PATH, encoding='utf-8')\n",
    "VACCINATION = pd.read_csv(GEO_PATH)\n",
    "LABELED = pd.read_csv(LABELED_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.360342e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>4,000 a day dying from the so called Covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.382896e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Pranam message for today manifested in Dhyan b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.375673e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>Hyderabad-based ?@BharatBiotech? has sought fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.381311e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Confirmation that Chinese #vaccines \"donï¿½t hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.362166e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Lab studies suggest #Pfizer, #Moderna vaccines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.351285e+18</td>\n",
       "      <td>1</td>\n",
       "      <td>Still want to take the #jab?\\n#PfizerBioNTech\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.377333e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>This time, Aerolï¿½neas flight AR1068 goes to Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.363344e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>#Covaxin effective against mutant virus strain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.372580e+18</td>\n",
       "      <td>3</td>\n",
       "      <td>Safe and effective. #OxfordAstraZeneca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.367507e+18</td>\n",
       "      <td>2</td>\n",
       "      <td>The day after the #Moderna #COVID19Vaccine... ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id  label                                         tweet_text\n",
       "0  1.360342e+18      1  4,000 a day dying from the so called Covid-19 ...\n",
       "1  1.382896e+18      2  Pranam message for today manifested in Dhyan b...\n",
       "2  1.375673e+18      2  Hyderabad-based ?@BharatBiotech? has sought fu...\n",
       "3  1.381311e+18      1  Confirmation that Chinese #vaccines \"donï¿½t hav...\n",
       "4  1.362166e+18      3  Lab studies suggest #Pfizer, #Moderna vaccines...\n",
       "5  1.351285e+18      1  Still want to take the #jab?\\n#PfizerBioNTech\\...\n",
       "6  1.377333e+18      2  This time, Aerolï¿½neas flight AR1068 goes to Mo...\n",
       "7  1.363344e+18      3  #Covaxin effective against mutant virus strain...\n",
       "8  1.372580e+18      3             Safe and effective. #OxfordAstraZeneca\n",
       "9  1.367507e+18      2  The day after the #Moderna #COVID19Vaccine... ..."
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELED.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for train dataset \n",
      "\n",
      "            Total\n",
      "tweet_id        0\n",
      "label           0\n",
      "tweet_text      0\n"
     ]
    }
   ],
   "source": [
    "def miss_val(df):\n",
    "    total=df.isnull().sum()\n",
    "    return pd.concat([total],axis=1,keys=['Total'])\n",
    "print(\"Missing values for train dataset \\n\")\n",
    "print(miss_val(LABELED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_link(string): \n",
    "    text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \",string)\n",
    "    return \" \".join(text.split())\n",
    "LABELED['tweet_text']=LABELED['tweet_text'].apply(lambda x:remove_link(x))\n",
    "TWEETS['text']=TWEETS['text'].apply(lambda x:remove_link(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "    \n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    " \n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) \n",
    "        \n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "    \n",
    "## Export pickle\n",
    "def make_export(tr, tt, file_name):\n",
    "    train_export = train[['id']]\n",
    "    test_export = test[['id']]\n",
    "\n",
    "    try:\n",
    "        cur_shape = tr.shape[1]>1\n",
    "        train_export = pd.concat([train_export, tr], axis=1)\n",
    "        test_export = pd.concat([test_export, tt], axis=1)        \n",
    "    except:\n",
    "        train_export['p_comment'] = tr\n",
    "        test_export['p_comment'] = tt\n",
    "    \n",
    "    train_export.to_pickle(file_name + '_x_train.pkl')\n",
    "    test_export.to_pickle(file_name + '_x_test.pkl')\n",
    "\n",
    "## Domain Search\n",
    "re_3986_enhanced = re.compile(r\"\"\"\n",
    "        # Parse and capture RFC-3986 Generic URI components.\n",
    "        ^                                    # anchor to beginning of string\n",
    "        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n",
    "        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n",
    "             (?P<path>        [^?#\\s]*)      # capture required path\n",
    "        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n",
    "        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n",
    "        $                                    # anchor to end of string\n",
    "        \"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "re_domain =  re.compile(r\"\"\"\n",
    "        # Pick out top two levels of DNS domain from authority.\n",
    "        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n",
    "        (?::[0-9]*)?                      # Optional port number.\n",
    "        $                                 # Anchor to end of string.\n",
    "        \"\"\", \n",
    "        re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def domain_search(text):\n",
    "    try:\n",
    "        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n",
    "    except:\n",
    "        return 'url'\n",
    "\n",
    "## Load helper helper))\n",
    "def load_helper_file(filename):\n",
    "    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n",
    "        temp_obj = pickle.load(f)\n",
    "    return temp_obj\n",
    "        \n",
    "## Preprocess helpers\n",
    "def place_hold(w):\n",
    "    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n",
    "\n",
    "def check_replace(w):\n",
    "    return not bool(re.search(WPLACEHOLDER, w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "  \n",
    "def make_dict_cleaning(s, w_dict):\n",
    "    if check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s\n",
    "\n",
    "def export_dict(temp_dict, serial_num):\n",
    "    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n",
    "\n",
    "def print_dict(temp_dict, n_items=10):\n",
    "    run = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        print(k,'---',v)\n",
    "        run +=1\n",
    "        if run==n_items:\n",
    "            break    \n",
    "## ----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1. Load Data\n",
      "1.2. Basic helpers\n"
     ]
    }
   ],
   "source": [
    "########################### Initial vars\n",
    "#################################################################################\n",
    "HELPER_PATH             = '../helper/'\n",
    "\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything\n",
    "\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "\n",
    "########################### DATA LOAD\n",
    "#################################################################################\n",
    "print('1.1. Load Data')\n",
    "good_cols       = ['tweet_id', 'tweet_text']\n",
    "if LOCAL_TEST:\n",
    "    tt          = pd.read_csv('../data/covid_vaccine_tweets_with_sentiment.csv', nrows=200000)\n",
    "    train       = tt.iloc[:-100000,:]\n",
    "    test        = tt.iloc[-100000:,:]\n",
    "    del tt\n",
    "    train, test = train[good_cols+['label']], test[good_cols]\n",
    "else:\n",
    "    train       = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "    test        = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')    \n",
    "    train, test = train[good_cols+['label', 'created_date']], test[good_cols]\n",
    "\n",
    "########################### Get basic helpers\n",
    "#################################################################################\n",
    "print('1.2. Basic helpers')\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "#good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "#bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "#global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "#global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "toxic_misspell_dict     = load_helper_file('helper_toxic_misspell_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 215023 | Known words: 12373\n"
     ]
    }
   ],
   "source": [
    "tweets = TWEETS['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "verbose = True\n",
    "global_lower=True\n",
    "tweets = tweets.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 183844 | Known words: 15179\n"
     ]
    }
   ],
   "source": [
    "if global_lower:\n",
    "    tweets = tweets.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 181874 | Known words: 15188\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "# Global\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "tweets = tweets.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "tweets = tweets.apply(lambda x: deaccent(x))\n",
    "if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 181874 | Known words: 15188\n"
     ]
    }
   ],
   "source": [
    "# Remove 'control' chars\n",
    "# Global    \n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 181874 | Known words: 15188\n"
     ]
    }
   ],
   "source": [
    "# Remove hrefs\n",
    "# Global    \n",
    "tweets = tweets.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 171590 | Known words: 15231\n",
      "ðŸŽŠó ´æ·±ðŸ¤ŽðŸ•¸ð’ðŸ¥ŽðŸ˜®à¸°ðŽæ®ºðŸŒ»ðŸ°å¾‹ðŸ»ì—¬à¬ªà«Œá´ç­’ðŸ™ð”ðŸ†•â›·ðŸ•¶ðŸ¥ƒðŸŽ¥ðŸ¤¤ð’‘ì§¸à¬—â˜ºð—‹ð‘¢à¬®à§ªð—¨ðŸ›°ð’æ™‚à·ƒà¸‹é›ªà°ªð“ƒå®¾ðŸšªðŸ’¤ðŸ™Œà²¥à²šð‘®Ù«ç¹ï¼’à¸ˆðŸ¿ðŸœðŸ´ðŸš¦ðŸ˜ŽðŸ¤¯ç³»ð’šðŸ“‰ðŸ˜—ðŸª™ð‘¬ðŸ‡ªì˜ðŸ“ëŸæŸ¬ðŸ’¡Û”ðŸš¶ð“·ðŸŽ£ðŸ‡´à°¨ð™–ðŸ¤™ðŸš—à°ð™¤ðŸ”ð—ˆæŽªà¸¬à´·ðŸ‘‰ðŸ’©ðŸ¥´ð˜”ðŸ¦…ð—¯ðŸ«ðŸ†‚ðŸ‡¬â«ðŸ¢å·®ðŸ—‘ðŸ’”ðŸŒ’âš“ðŸ¾ðŸ«ðŸ¥°à¸›ðŸ’„ð—¿ð‘ì›ð˜–ë“œðŸ’¥ðŸ“ˆðŸŽˆðŸ˜±ð™šâ²ðŸŽ¨ðŸ‘µå¤šà®ˆðŸŽƒðŸ’—ðŸ“žç§ðŸ·ðŸ˜µà®£â„…ð—¤ð“¾ìž¬à¤‘ó £ð‘›ðŸ©ºðŸ¥¯ð˜…ðŸŸ©ð¯ðŸ’†ë ˆâºâš–ðŸ ð–¦ð™à®™ðŸŽ®â„¹ðŸ”‘ðšˆðžâæŸ“ðŸŒ…ç¦ð™ƒðŸŒ¬ððŸ†ðŸ¤•âœ‚â—†í•˜ð”ðŸš€ðŸ˜¤à¤”ð‘·à¬²ðŸŸ¡ó ¿ðŸš«â–«èŽªçµ„â˜›å…ð—¬ðŸ¤‘ð’†ðŸŽ“ðŸ¤²ðŸ§à¬…ðŸ©¹à°Ÿð’•ðŸ‡³ð‘Žâ»ì•¼à¹„ë…•ðŸ¤®âš™ðŸ§»ðŸ›£ðŸ¦•ð£ðŸ­à®œðŸ”±ðŸ“»ë‹ˆðŸŒà·„âºðŸª‚ð’ð™„ðŸ‘†å—ðŸ”²ð€ðŸª¶ç·¨à²¸ðŸ˜¦ð‘­âŒší•´à¬¸ðŸ˜ªì™”ðŸŽ­à¹ƒð™¡ðŸ”½ðŸ–•ì¹´ðŸ›ã‚©ð‘¤ì—´ð’‹ððŸºà°—æ˜¯ã€‘ç½ë‹¬ðŸ˜£â™Ÿà¸ŸðŸ¦°ðŸ¤¬ðŸ¦¬é˜²àª¾ðŸ’…ðŸ–¼ðŸ†™à°…ð’‚ð™Žð™¸ðŸ‘¹ð«ð™®É¢è¬ð—¡å¥½ðŸ—ð™¦ðŸ‘ðŸ¤žðŸª’ðŸ˜¬ðŸ˜©ë‚œÂ€à°‡ðŸ¤Ÿà°®ðŸ«ð‘ ðŸ”§ðŸ‘±à¬–ðŸ•‰ðŸ’®â›¥á€›ðŸ˜½ðŸ¦Žðš’åí”¼ð›ðŸ’‹ð™¢â˜£ðŸ‘ð—›ðŸ•ðŸâ‡ð°ðŸ ðŸ¨ðŸ˜¡âš•ç¨®ðŸ¥§ðŸ«ð™­âž¤ðŸ’§â¶ðŸ’¬ðŸ“–ðŸ¤©ðŸ†â›„ðŸ¦žðŸŒ‹âœ‰ì—‘à®·ðŸ‡©ðŸ“£ðŸ‘«ðŸ§‘ðŸ›’ðŸ’Žâ˜˜ðŸ¥¬ðŸ˜·ðŸ‘ˆè¨€âžŒðŸ‘„ðŸ¤ èŽ«ó ®ðŸ’ðŸŒ€ðŸ”ºðŸŽ‚ðŸ›¥ðŸ‡®ðŸŽµà±‚à¸‚ðŸ§¡á´ ä¼ð’”à¥‰ðŸ¤¥á´„ð’Œð‘†ðŸ‹ðŸ“‹ì—ðŸŒðŸŒ³ð–§â €ðŸšŒæ‹âˆ™â”ðŸ¸à°¦ðŸ‡¼à¤ˆà°­ðŸŸì†Œà¶©ë„ðŸŽ™ë³´ðŸ¦á´‡ð™‹â€¼æ‰“àµ†â˜¡ðŸ–ðŸ”—ðŸ‘‡ã€½ðŸŽ‘ð‘ªð‘˜ðŸ¤”ðŸŒ¿ðŸ˜’Êð˜½ð—´â”ŒðŸ–¨ðŸ˜‹ðŸ¦åŒˆçˆ±ð™³ð—¥ðŸªðŸŒ¸ðŸ›ŽíŠ¸æ¥ðŸŒðŸŒ²å…°ðŸ¥±ð“â–¡ðŸ¦ ð–¨ð‘–ðŸ’ƒíŽ˜ð¬ðŸ…¾ð˜“ç¶­à¬¥ðŸ—£ðŸ–ðŸ‚ðŸ”ðŸŽðŸ‘æ¯…ðŸ‘¦ð˜ð˜œðŸ§ðŸ”®è¯à°œâ™’ðŸ¼à²†ðŸ“Žà¤¢ó ·ðŸ¥‡ðŸ’ºå‰¯ÊŸðŸŒ®ë¡œážšðŸ¥¶ðˆðŸ—³ðŸ‘ŽðŸ”¹å…¥à¸Žà§¦ì„¸âœ”ðŸµå¤‰ðŸ‘­ìš”ðŸ”¶àª¸ðŸ’‰ðŸ‘™à´¨ï¾Ÿí‚¤ðŸ…°ðŸ“˜æ²¹ðŸ¥¼à§«ðŸ¦¶ðŸ˜ðŸ·è‡ªðŸŽð˜ŒðŸ¤­ð‘“ðŸ˜‰â€›ðŸ“·â­ð—”ð˜ðŸ“šðŸŒµà®¸ðŸšðŸ’•ð™ð’ŸðŸ˜¯ðŸš¯à®±ð—œà¬¤à¬™â†¬ðŸ½ðŸ‘ºà¬¦ðŸ’–ðŸ¦†ðŸ”¸ðŸ§¼ðŸ„ðŸ†šðŸ˜•âƒ£ðŸš”ðŸ™‡áž€ðŸŽªæ®˜ðŸ¤ªé‚£âž”çµµð˜‰ðŸ‡­ð˜‹à°šà¹‚ðŒðŸššà¬¨à¸æ´¾ð™¾á€†ðŸ‘¶à¶œðŸ’“ðŸš´é’ˆðŸ’€ðšœðŸ”Šð–¼â˜¹ðŸ—»âœ‹ðŸ§•ðŸ‡µðŸ‘‹ðŸ™ˆðŸžðŸ¥¾à´—ð™‚à´°ðŸ¤“ð–¿àª•à¸’æ›ðŸ˜³à²—ð“»ðŸ·ðŸ˜ŒðŸ”ðŸ¥²ðŸ‘®ð‘¹ç£ð—­ðŸ…²â˜à¤ƒà¸ ðŸ‡±åˆð¢ðŸŒ–à³¦ð™¨ðš‹ð‘’ðŸ‘»â˜ðŸ¦–ðŸ›à²§âš¾ð’–â—€ðŸ‘¸ðŸ¥ðŸ‡§ì¼â—ðŸ‘ðŸ“²ðŸ’°ì¶”ð‰ðŸ™à°µðŸ¤¡ðŸ¥’â†˜à²®á€—â¬›à­¯à¶…ðŸ¤Œà·…à·™ðŸª±â””ðŸŽ¶ðŸ•’ðŸ—¾ËŒð—™à·ðŸ¼ðŸ—ðŸ¸ð™ë†€ðŸ¨í™”æ¸¯é¡â†™ðŠðŸ»ðŸ§£ð‘²ðŸ˜…ðŸ’¼à¬­â™âŒ›ð®å…´â·à§§à²¹ë¦¼âš½ðŸŽºë‹¤ðŸ§¨ðŸ¥·ðŸ³ðŸ¦Šð±ð„ðŸ–ŒðŸ¤ðŸ¦ˆç‰™ðŸŽ§ðŸ‘°ð‘™å…ˆð™ ðŸ½ï»ŒðŸ’´ðŸ«‚ðŸŽ¤à¸âžà¶Ÿâ˜ƒðŸ”µï¼žð‡ð‘¨ðŸŒŸðŸ“®ð“ªì¡°ðŸºð˜¾à­§ð´áž¶ðŸ†˜ë·”èˆˆðŸ––ðŸ•ðŸš™â¤ð™¯á€±ð— ð–ðŸ§¿ðŸ˜ðŸ¥…ð“½ðŸŽ¯ð–­ð’Žð—¹ðŸ‘¾ð˜¥ðŸ˜‚à²³ðŸðŸ‡¶ðŸ–â–ˆç§‘âžŽà¸â„ŽðŸ¿åœ’æ ªðŸ¼à¬³ðŸ¤—ðŸðŸª„ðŸ“¢ðŸŒ´âƒðŸ§§ð¿ðŸŒ¼à¶´à¶‘ì•„ð™±æ ¡â™‚ðŸŒ±ðŸˆà´Ÿà¤›ðŸ§µà¯ŠðŸ‡ðŸŽ‰ðŸŒ™â˜„ðŸ•ðŸ˜â™ˆðš›ðŸŒðŸªá´˜à²…ðŸµðŸ”™ðŸ—½ðŸ˜“ðŸ¿ð™´à¶¶ðŸšºÂ”ðš”âœ…æ€âš˜ðŸ®ðŸŒ•à®“ðŸ›¸åˆ©à²²â¬‡ï¿¼ðŸ’é›†ðŸ¤œå‹ŸðŸðŸ”ˆðŸ¥—ðŸ•ð“°à¸…ðš£ðŸ‘¼â•à¤˜ðŸ—ð˜„ðŸ¦²ðŸŒ¾ðŸš‘à§°ð‘³â—¾ðŸŒŒå“’ì§€ðŸ”˜ðŸŽ¬ðš à®Šð™Šð‘«ðŸ’¨å°á€šæ„ŸðŸ€ðŸ“ðŸ¦Ÿå°”ð’Šó ¥à°‚ðŸŸ ðŸ™ðŸ©°á€’ðŸ’šðŸ¦„à®†ì†”à¸šâœ³ðŸ”ðŸ’›à·‘ä½ ðŸ˜”ï¼ƒð™…íŒ…ð™‰ðŸŒºìƒðš…ðâœðŸ¥‚ÚˆðŸš©ðŸ’¦ðŸ‡¹à¬§â£á´ð™°â˜ è¶Šà·‚ðŸ¦‹â˜Žâš°ðš˜à°·ã€ä¹ˆð’›ðŸ‘”ðŸ˜«ð’˜á´€ð“­ðŸ™ðŸ¤ðŸ’ðŸš¨ðŸ§”å¸‚á—©à²•ð—žâ–ºâž–à¶§ðŸ•–à¸˜â€ðšðšðŸŽ†ì¶•å“ˆðš•ð’ð‘©ðŸ“ºðƒéšœå‰‚à«‹ðŸ™„ðŸ˜¶ð”‚ð˜è¼ªðŸ¥¤à·ð—£ðŸ—“ðŸ“ƒðŸ‰ðŸ¶ð‘´ó §ðŸ§ƒð“ðŸŒ›í˜„â—¡â›µðŸŠð—‡ðŸ¤¸ë²ˆâ£ðŸ”´âœ´ðŸ”·ðŸ¥”ð©ð™µà°¬à³ðŸŽðŸŸðŸ±ç‚¹ðŸ§Ÿâ–‘ðŸ’·ðŸ°ðŸ©³ðŸ±à´£à¯†ð’…à°²Û°â˜€å¿œð†â“¦ð—¸æ··ðŸŒšðŸ˜ð™ë…„ð—±ë°©ä·‰ð‘ºðŸ“Œå…±ðŸŒ”ðŸŒ«ð™ˆðŸŒ¡ðŸ®ðš‘ðŸ´ðŸŽ¦ðŸ¥³â„ƒâ™¨ðŸŒ·âœˆðŸ”žæ¯’à²¬ð™ªç´âš—ðŸ“³ðŸ‘’ê·¼à¤“å¸¸ð“«è°¢ðŸ’ ðŸ“‘â¦é„‰ðŸ˜ƒðšŠð‘ðŸŽðŸ˜ à¸¤ã…‹è‹—à¤Šá´œâ€¤ä¸šðŸ§™ðŸ†—ï¿½ðåœ–à¬‡à´¿ðŸ¦¸ðŸâ›©ðŸš½ç‚Žð™¥âš«ð…ðŸ…¶ðŸŒžæ©ðŸ‹ð—–ð—ŸðŸŒ¹ð§ð‘Ÿæ­¸æ­»ðŸ˜†à¤‡âž•å…­ðšžðŸ¦‰ðŸ‘¢ðŸŒˆï¬ƒâœ¡á€¸ä¸ºå®Ÿâ©ðš—à°•à·†ð™‘ðŸ¥µä»€ðŸ˜°ð˜¿ã€°ðŸ”¼ì •æš—ðŸ€ðŸ•ŒðŸ’™å¼ºð¸Û´ð– ðŸ˜œäº¡ð‘°ðŸ˜˜ðŸ¸ð˜¯ðŸ¤°ðŸ’˜ðšŽà­®à²¡ðŸ‘¥ðŸ‘ð™ðŸ™†Û±à±ðŸ˜¹â›…ð˜€ðŸš§ð™›èµ·ðšðš„â¤´ì—˜ð™¿æŒ‡ð“£ðŸ©¸ðŸ§Žì¦ˆë©ˆðŸŠà°¯ðŸ›«ð—°ðŸ”¥ðŸ’­à¶‡ðŸ¤§â“¿à´¯à¹ð’‡á´‹å‘¨æ‹‡à²¶ðŸ¦‡â˜”áž–ðŸ¤›ðŸ§²ðŸŒâš§ì•ˆà² à¥Œë‚˜ðšðŸ’¯ð–®æ»¨ðŸ˜ðŸ¦¹ð–ªðŸ‘©à­ŸðŸ”†ðŸŸ£à¸³ðŸ¬ð“µðŸ›µê·¸ð—¼ðŸ“½ðŸª“ï½œå·¥ðŸ–±ðŸ¤±à®’â–’ð˜ŽðŸ»à°°ðŸ”Žë¼ð“¿ðŸ‡«ðŸ²ðŸðš–à­€ðŸ“¸ðŸ¥ðŸ—’å›žðŸ«€à°¸ðŸŒŽâœŒà¯‚ð’ƒð²ð’ˆâ¬œà§®à°«ðŸš®ðŸŒ ðŸ¤ð™§ðŸ•™à§¨âš±ðŸš¢ðŸ“ŠðŸ‡²ðŸ§’ðŸŽ¢ð–¾ë¯¸ç«–ðŸ§©ðŸ†“à¬£ð™žðŸ‡¯ð˜¼à®…ðŸ’»ð™£ðŸ‘§ðŸ¥ˆðŸª²ðŸ˜¢ðŸ¦©ìŠ¤à­¨à´­á€€ðŸ˜ˆð”ƒðŸ§˜æ–½ðŸ—žçµ²ðŸ§¤à¸¨è‚²ðŸâ˜¯ðŸ¦âƒ¤ð­à¶­à¬œðŸ¤·ð™ðŸ§ªð™¬ð™«â“ðšà°¡ðŸ¥ðŸ–‡â–“ðŸ”‹ðŸ›ë°ë¶ˆâ¬†ð‘‡âœ’ðŸ‡¸â¬…ðŸŒŠð‘œð‘¡ðŸ¹á´›ðŸ‘—ð—©â›ˆâ™¡à®´ðŸŽ—à¬•à¬¯â˜•á´…âœðŸ‘‘á€„éžð˜ƒð™ŸðŸ­ð—•ð“ðŸ¤¢ð—ªâ©ðŸ“±ðŸ…µðŸ˜‡ð˜¢ðŸ”„â„ðŸ¤˜ðŸ§«ðŸ’ŒðŸ› ð–«à¸©ðŸ˜€ðŸ°ðŸ§¬ðŸ””à²¦ðŸ„ðŸŒ„ð—»ð™™ðŸ¤£à¥ð™²êµ­â›½áŸ£ðŸ“ ðŸŽžì–´ð—§ðŸ’à¸–ðŸ‘ƒà®‰ðŸ˜¨ðŸ›‘ðŸ›¡ðŸš‚á€…ðŸ”«åž‹à¬ŸðŸŒ˜å½©ð—ðŸš¬ðŸ§“ðŸš¹ðŸ—ºðŸ”ŸðŸ¦¾ðŸ™‰â–½â±ðŸ’²ðŸ˜žð™¶ðŸƒðŸ†ðŸ§ðŸ“¦ðŸ“¬ðŸ§ ðŸ§à²¯ð’‰ð‘µðŸ‘´ðŸš²à³‚â”ðŸ…¸ðŸ™‚ðŸ¾ð¥â™»ðŸ¤¨à¤«ðšŒå¹¼ðŸ‘½ðŸ–à¸†ðŸ’¶â˜¸âŒ˜ðŸ¤«ð™—ðŸ”»ðŸª†àª¥ðŸ˜™ð——ðŸŽ·ð‘”ð™©ðŸ·ð—«à´•ð˜‚âŒâœ“Ê™é‚æ‚£æœªæƒ‘â˜®à¶‚à¬¬à²–ðŸºðŸ°ð–ºè­²âš›ðŸ‡»ðš‚æŽ¥ðŸ”¦âœ¶ðŸ”•ðŸ­ðŸ‡°ðŸ§šðŸ‡¿åŠ‘ä¾†ðŸ˜»ðŸ£à´±ðŸ¶ð‘ˆðŸŽ¼ðŸ”ƒðŸ”¬â˜œðŸ³ðŸ¹ðŸ©à¶¢à¬†ðŸ¤³ðŸ‘€â˜ªðŸ™‹ð³ð™½ðŸ”¯â”˜ðŸ–ç–«å¯¨ë§ˆá€ŸðŸ©ð ðŸ§‹âž¡ð—³ðŸ‡½è±¬à´žð—¶ðŸ’žðŸŽ³ðŸžð‘»ðŸŽ„æŒ™ð–¡éºðŸŒ¦ðŸ¯ðŸ–¤ð‹ðŸ½è‚ºðŸŒ§ðŸ…à¶¯ðŸ•¹ðŸ˜‘á€”ð™œðŸ¦±ðŸ’¸ðŸ•“ðŸé†«ðŸ”Œâ™€è£½ðŸ’£ðŸ˜–ð—²ð™»ð˜•ðŸç—…ðŸ˜„ðŸ¤ðŸ’ŸåºœðŸ™…ðŸ…½ð’„å­”ð—šðŸªðŸšœðŸ©ðŸ”ªð™¹à´¤ð–£ðŸ¤¦ë¦¬ðŸ§¢â—¼ð—˜à¸Šà²‚ðŸ¥ºðŸ“¶ðŸà²µâœ¨ðŸ¦§ð—®ðŸ’é™¢ðŸ§ˆð“¶ðŸ¤–ð‘âš æ¸‹ð“¼ðŸ“¹ä»¶â›³ðŸŒà·œë…€à¸¿ðŸ‘â˜žæ”¯ðŸ“è²ç±³ðŸª¦ðŸ¦ºåðŸ’¹ðŸ˜§ðŸ˜ØŸð“±ð˜‡â›‘ðŸ¾ðŸ¦³ðŸžðŸ™€à°‰à¸‰ç¨šà´¾â­•ðŸ‡¨âœŠðŸ¥¢ðŸ˜¥â›”ðŸ“©ðŸ‹åŒ»ðŸ¤¾ðŸ‘ªàª¨à¤ â‰à¬¾ðŸ”åœ³ð˜™ó ³ðŸ‘à¬°ðŸ˜ºðŸŒ¨à¸“ðŸ¦à¬¡ðŸ‘•ðŸ˜Ÿð“¸ðŸ¥¸ðŸ—¯à¤à°§â ðŸ“°ðŸƒðŸ¿â˜‘ðŸ’œð–³àµ»ðŸ“„ð—…ðŸŸ¢ðŸ€ðŸ§€ðŸ¡ð’“àª¿ðŸ¥Šð™˜á€žðŸ‡·ðŸ‘Šðœà¯‹ðŸ™ŠðŸ¥ðŸŒ¤ð—µâ—»ðŸ‘‚ðŸŒÊ˜è—ðŸ’«ðŸ’‘Â¯ðŸ¦¯åŸ”ðŸ’ðŸ“¡ðŸ‡ºðŸ‘¿ë””ðŸŽ€ð˜ˆðŸ³ð—ºà³•ð˜—ðŸŽ¸ð˜†à°¤ðŸ”¨ðŸ”³à¸”à¬·ð¤à´´â¤µð•á€¬ðŸ˜²ðŸŽ‡ðŸ‘£ðŸ‘¯á€»ðŸ¾ë‹¨ðŸš˜ð“¯à®‡à¬«â°àªµëž‘ð™€à­‹â³å®ŒðŸ”œé€£ðŸ“…ðŸ‡¾ðŸ§›ðŸ˜šâ€£ðŸ»ðŸ’µðŸ’Šà²ªðŸ•Šð‚à´‚à¸„ðŸó ¬ðŸ‘¬ð—¢á€ð—‰ê½ƒç‰©ê³ ðŸ¥½à¶±ðŸ•ðšŸà¸œð“®ðŸ§³â‡©ðŸâ™°ðŸ¤’ðŸ›‚ðŸ¤šâš¡ð‘ƒæ¶ˆðŸ’‚ð—¦ðŸ˜­ð“²ðŸ˜¼ðŸ¢ðŸ˜›ððŸ¡ðŸ•‹ðš¢ðŸ›¬ðŸŽ©ð‘¶ì™€ê°•ðŸµà²¤ðŸ˜¿âš”ðŸ’¢ó ¢âŽâ¬áðŸ‘Œâ–¶ì½”ðŸ‡¦ç¸¾ðŸ˜´à¶¸ðŸ•ºÚ‘ðŸ“ðŸ™ƒðŸ¥‰ð·ðŸ…·ðŸ’ªà¸«â€ŒðŸª³ðŸ˜Šâ˜¢íƒ„ð—ð¦ðŸ‘¨ð¨à¶·ð¡ðšƒâ†—à®žç•°ðŸ˜¸ð—½\n",
      "127882 --- \n",
      "917620 --- t\n",
      "28145 --- \n",
      "129294 --- \n",
      "128376 --- \n",
      "119949 --- l\n",
      "129358 --- \n",
      "128558 --- \n",
      "3632 --- a\n",
      "119822 --- o\n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 157794 | Known words: 15397\n",
      "Â·ãƒ†Ð´â– ãƒ§ãŸà¯ˆãƒ­à¤¨åŠ ãƒØ´ÑŒÚ©à²¾â€ ãƒ˜Ðµà¸•ØªÎ¼â‰ˆâ”‚ÛŒã‚¤ãƒ©å±±ã‚¢Ø¨à¤²à¸¡ØŒãƒŠà¤ªã†ã‚»äº”ÙŠâ‚¬à®¿à¦²ã‚‚Î±à¶šà¤–à¤¥à®³Ð°à¦¾å¾·è°·Ø­Î´â„–â€žØ¶Ë¢Ù…Ø¯ã¿à¤¹à¦†å¹´ã€‹ãƒ›Ø¸Ïƒâ€ºã‘ã‚³à¸£â†Ï‰à®¤à¸â˜†Ù‚à¤µè‹±è¡—à¥€ç›¸ã›æ­£Ð¸Ù¹â‚¹â€¦ã—å…ƒÑ–à¸²à¤®É´ã«à®µÎ·Ø°à¤¡ï¼à¦¯ï¼šâ†’ãƒªà·€ï½žà¤¿Îµà§‡ÏˆÙ†ã¾â€‘ÐºÑ‡Ñƒå›½Î¿à¸¥à¶ºà¦¬é«˜æ—¥ã“æ”¿åœ‹Øµé¦™à®¾áµ—Ð¿Î»ãƒ¥à§€Ï†à¤¾à¤­ã„ä¿âˆšã¯à·ãƒ¼à®šãŠÑØºãƒ³Î¸à¤‰â—à¸ªæˆ‘à®¯ä¸€à¦¿à¤¸ì˜à®•é™³ã¨Îºà¦¤à¤¯à¦®ãƒ„Ú†Ð¼Ø©å­¦ï¼‰Ø³à¤—ÚºäººØ²ã€‚à¯ãƒ»Ñà¤¬ã‚Šà®¨ãˆã‚â™ ÏÉ¡Ñ…ãƒˆÐ³æ­¦ã‚¦Î³à¦ªå®¶ã‚ˆãƒ«ì´à¥‹à¶½âˆ†ã€ŒÙ‡ä¿¡à¸¢à¶»à¸‡â¿ã‹à¤šà¤·Ø¡à¦­ãƒ¯ä¸‰ÑŽâ„¢ç”Ÿä¸Ø§Û’Ï…ï¼å¥³à®©Ùæ˜Ÿà¤¶Ð»à®°å­à¦¦à¤œãƒžâ–ªØ¬à¦¨â™£ã‚¹ã‚µä¸­ã®à¸™á€™Ø«â˜…à²¨ã‚‰à¯‡Ð½å£«à¹€å®šà¤£Ï‚à®ŸÐ·å—à¸§äºŒï¼ŸÚ¾à¸­ç¾ŽÙ„à¦•ã‚“ã•Ûà¦›à¤°Î½ÙƒÐ¾à¦°à¸—ãƒ¬æ±ã™à¤§ã‚«Ù‰Ù¾ï¼Œï¼ˆà¦œà¦§â…“áµˆà¤†Ð²à¤¤â™¦à¦¸å¤§ã‚±Î²â‰¥ì‚¬â™¥åˆÏ€Ñ‚à¥¤ã¦à¦¹ãƒ’Ùˆà¦–à¤æˆÎ¹Ø®à®²Ê€ÉªÑ€à¤ŸÚ¯à¦‰ãƒ¢ã‚¯à¦¶â€¢ÊŒà¸žãƒ ãƒäº‹æ–°Ð±Ð¶ãƒƒã€ŠØ·ãƒ£ã‚·æ–‡à¤¦à¦·à¤…ãƒ•æœ¬ã‚ªà²°Ø¹äº¬Ï„à®®æ¼¢ï¼Ñ‹à®ªØ±â”€à¤•ç›®\n",
      "183 --- \n",
      "12486 --- \n",
      "1076 --- \n",
      "9632 --- \n",
      "12519 --- \n",
      "12383 --- \n",
      "3016 --- \n",
      "12525 --- \n",
      "2344 --- \n",
      "21152 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = 'Â·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 157794 | Known words: 15397\n"
     ]
    }
   ],
   "source": [
    "# Remove html tags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if ('<' in word) and ('>' in word):\n",
    "        for tag in html_tags:\n",
    "            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                temp_dict[word] = BeautifulSoup(word, 'html5lib').text  \n",
    "tweets = tweets.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 157794 | Known words: 15397\n",
      "https://www.bitchute.com/hashtag/moderna-pfize --- word_placeholder[bitchute.com]\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it)) \n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "    \n",
    "for word in temp_dict:\n",
    "    new_value = temp_dict[word]\n",
    "    if word.find('http')>2:\n",
    "        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n",
    "    else:\n",
    "        temp_dict[word] = place_hold(new_value)\n",
    "            \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 157789 | Known words: 15397\n",
      "now/perfectclippingpath.ab97@gmail.com/ --- word_placeholder[url]\n",
      ".com/video/9hipj --- word_placeholder[url]\n",
      "@pfizer/justincloughqlgbtiq@gmail.com/#chemistry --- word_placeholder[url]\n",
      "ttps://www.bbc.com/news/world-asia-china-57817591 --- word_placeholder[bbc.com]\n",
      "profile: --- word_placeholder[url]\n",
      "://www.reuters.com/business/health --- word_placeholder[url]\n",
      "#www.thekhybermail.com --- word_placeholder[url]\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "\n",
    "for word in temp_vocab:\n",
    "    url_check = False\n",
    "    if 'file:' in word:\n",
    "        url_check = True\n",
    "    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "        if 'Aww' not in word:\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone in word:\n",
    "                    url_check = True\n",
    "                    break            \n",
    "    elif ('/' in word) and ('.' in word):\n",
    "        for d_zone in url_extensions:\n",
    "            if '.' + d_zone + '/' in word:\n",
    "                url_check = True\n",
    "                break\n",
    "\n",
    "    if url_check:\n",
    "        temp_dict[word] =  place_hold(domain_search(word))\n",
    "        \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 157788 | Known words: 15397\n",
      "[19:40:35] --- [19:4ðŸ˜‡5]\n",
      ":)) --- ðŸ˜\n",
      "[11:20:36] --- [11:2ðŸ˜‡6]\n",
      "[10:32:22] --- [1ðŸ˜‡2:22]\n",
      "[10:30:50] --- [1ðŸ˜‡0:50]\n",
      "[10:35:37] --- [1ðŸ˜‡5:37]\n",
      "[14:00:38] --- [14:0ðŸ˜‡8]\n",
      "[08:40:31] --- [08:4ðŸ˜‡1]\n",
      "[04:00:33] --- [04:0ðŸ˜‡3]\n",
      "@a_girl_isno_one --- @a_girl_isnðŸ˜®ne\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if (pict in word) and (len(pict)>2):\n",
    "                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "            elif pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 157788 | Known words: 15397\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate emoji\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in tweets for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 150719 | Known words: 15428\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "        if (Counter(word)['.']>1):\n",
    "            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "        if (Counter(word)['!']>1):\n",
    "            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "        if (Counter(word)['?']>1):\n",
    "            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "        if (Counter(word)[',']>1):\n",
    "            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(tweets, local_vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 150683 | Known words: 15428\n",
      "#__o --- #o\n",
      "#_o___ --- #o\n",
      "#_i_ --- #i\n",
      "@g___m____m --- @gmm\n",
      "#___ --- #\n",
      "_____ --- \n",
      "#o___o --- #oo\n",
      "#_____ --- #\n",
      "#e___ --- #e\n",
      "#_o --- #o\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "        temp_dict[word] = re.sub('_', '', word)       \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 150660 | Known words: 15428\n",
      "\"\"\" ---  \"   \"   \" \n",
      "$$$$$$ ---  $   $   $ \n",
      "=============== ---  =   =   = \n",
      "#### ---  #   #   # \n",
      "$$$$$$$$$$ ---  $   $   $ \n",
      "******* ---  *   *   * \n",
      ":::: ---  :   :   : \n",
      ":::::: ---  :   :   : \n",
      "***** ---  *   *   * \n",
      "+++ ---  +   +   + \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(3)])\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 150657 | Known words: 15428\n",
      ":) --- ðŸ˜\n",
      ";) --- ðŸ˜œ\n",
      ":/ --- ðŸ¤”\n",
      "\\o/ --- Yay, yay\n",
      ":} --- ðŸ˜\n",
      ":( --- ðŸ˜¡\n",
      ":* --- ðŸ˜˜\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if pict==word:  \n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 141978 | Known words: 15486\n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "# Global\n",
    "chars = '()[]{}<>\"'\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 139858 | Known words: 15503\n",
      "study/research --- study / research\n",
      "#oxford/az --- #oxford / az\n",
      "sherwood/coursey --- sherwood / coursey\n",
      "6/26, --- 6 / 26,\n",
      "reaction/side --- reaction / side\n",
      "17/9/21 --- 17 / 9 / 21\n",
      "hcw/flw --- hcw / flw\n",
      "research/ --- research / \n",
      "kent/london/essex --- kent / london / essex\n",
      "2/20 --- 2 / 20\n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 139762 | Known words: 15511\n",
      "#chulabhorn_royal_academy --- #chulabhorn royal academy\n",
      "ghaziabad/noida/lucknow --- ghaziabad / noida / lucknow\n",
      "intellectuals/media/burocrats. --- intellectuals / media / burocrats.\n",
      "#trilateral_commission --- #trilateral commission\n",
      "#positive-cases-covid19 --- #positive cases covid19\n",
      "covid-19,vaccine-induced --- covid 19,vaccine induced\n",
      "representatives/senators --- representatives / senators\n",
      "astrazeneca/covishield --- astrazeneca / covishield\n",
      "after-you-are-hospitalized --- after you are hospitalized\n",
      "takinginmade-in-india --- takinginmade in india\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '_' in word:\n",
    "        temp_dict[word] = re.sub('_', ' ', word)\n",
    "    elif '/' in word:\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    elif len(' '.join(word.split('-')).split())>2:\n",
    "        temp_dict[word] = re.sub('-', ' ', word)\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(tweets, local_vocab); \n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 136622 | Known words: 15512\n",
      "#chennai? --- word_placeholder[#___chennai] ?\n",
      "#mythreesons --- word_placeholder[#___mythreesons]\n",
      "#covidhair --- word_placeholder[#___covidhair]\n",
      "@dagrandinetti --- word_placeholder[@___dagrandinetti]\n",
      "#shared --- word_placeholder[#___shared]\n",
      "#kalpkrizlerisalgÄ±nÄ± --- word_placeholder[#___kalpkrizlerisalgÄ±nÄ±]\n",
      "@dcm50 --- word_placeholder[@___dcm50]\n",
      "#shenzhen. --- word_placeholder[#___shenzhen] .\n",
      "#repmtg --- word_placeholder[#___repmtg]\n",
      "@soticova, --- word_placeholder[@___soticova] ,\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags (add username/hashtag word?????)\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (len(word) > 3) and (word[1:len(word)-1].isalnum()) and (not re.compile('[#@,.:;]').sub('', word).isnumeric()):\n",
    "        if word[len(word)-1].isalnum():\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:]) \n",
    "        else:\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + ' ' + new_word[1:len(word)-1]) + ' ' + word[len(word)-1]\n",
    "\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 136618 | Known words: 15513\n",
      "#mkultra_microchips_ --- #mkultra_microchips\n",
      "@bridget_joy_ --- @bridget_joy\n",
      "@_matty_h_ --- @_matty_h\n",
      "must_ --- must\n",
      "@ace_trader_ --- @ace_trader\n",
      "@_spike27_ --- @_spike27\n",
      "@sanju_verma_ --- @sanju_verma\n",
      "@ash_stewart_ --- @ash_stewart\n",
      "@karinhosa__ --- @karinhosa\n",
      "#pakistan#_ --- #pakistan#\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[len(word)-1]=='_':\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1]!='_':\n",
    "                new_word = word[:i]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 136618 | Known words: 15513\n",
      "_- --- -\n",
      "_accumulation --- accumulation\n",
      "_after --- after\n",
      "_dose2 --- dose2\n",
      "_vitamin --- vitamin\n",
      "_tx --- tx\n",
      "_he --- he\n",
      "_say --- say\n",
      "_with --- with\n",
      "_same --- same\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore \n",
    "# Local\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[0]=='_':\n",
    "        for i in range(len(word)):\n",
    "            if word[i]!='_':\n",
    "                new_word = word[i:]\n",
    "                temp_dict[word] = new_word   \n",
    "                break\n",
    "data = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 111261 | Known words: 16000\n",
      "14-06-2021, --- 14-06-2021 ,\n",
      "vacccines, --- vacccines ,\n",
      "somewhere! --- somewhere !\n",
      "centro, --- centro ,\n",
      "produced, --- produced ,\n",
      "60$ --- 60 $\n",
      "play, --- play ,\n",
      "extension. --- extension .\n",
      "protected, --- protected ,\n",
      "jr.: --- jr .:\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word),0,-1):\n",
    "        if word[i-1].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 107810 | Known words: 16029\n",
      "@cpft_nhs --- @ cpft_nhs\n",
      "@green_bird007 --- @ green_bird007\n",
      "'they --- ' they\n",
      "'diplomatic --- ' diplomatic\n",
      "#pfizerbiontech-made --- # pfizerbiontech-made\n",
      "@dpol_un --- @ dpol_un\n",
      "@yusuf_ch --- @ yusuf_ch\n",
      "@mansi_jain123 --- @ mansi_jain123\n",
      "@raw_em_md --- @ raw_em_md\n",
      "@prc_amb_uganda --- @ prc_amb_uganda\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word     \n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 107810 | Known words: 16029\n",
      "w.e.b --- word_placeholder[web]\n",
      "i.v.o --- word_placeholder[ivo]\n",
      "u.s.a --- word_placeholder[usa]\n",
      "f.a.i.r --- word_placeholder[fair]\n",
      "p.u.s.h --- word_placeholder[push]\n",
      "w.e.a.r --- word_placeholder[wear]\n",
      "a.k.a --- word_placeholder[aka]\n",
      "f.d.a --- word_placeholder[fda]\n",
      "d.o.n.e --- word_placeholder[done]\n",
      "r.i.p --- word_placeholder[rip]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "            temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "        else: \n",
    "            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 107810 | Known words: 16029\n",
      "f*%&gt;\\!g --- f*%&gt; / !g\n",
      "s\\se --- s / se\n"
     ]
    }
   ],
   "source": [
    "# Convert backslash\n",
    "# Global\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]    \n",
    "temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(tweets, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 107785 | Known words: 16029\n",
      "goal--100 --- goal-100\n",
      "----------------------- --- -\n",
      "----------- --- -\n",
      "vaccines.--25 --- vaccines.-25\n",
      "record--&gt;i --- record-&gt;i\n",
      "1.54$---&gt;19 --- 1.54$-&gt;19\n",
      "vaccines--#sinopharm --- vaccines-#sinopharm\n",
      "ca_osg--&gt;i --- ca_osg-&gt;i\n",
      "34$---&gt;3,40 --- 34$-&gt;3,40\n",
      "yey--done --- yey-done\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 86459 | Known words: 16451\n",
      "dose2:100 --- dose2 : 100\n",
      "dose1:2100 --- dose1 : 2100\n",
      "2:21:00 --- 2 : 21 : 00\n",
      "13.07.2021 --- 13 . 07 . 2021\n",
      "to12-16weeks --- to12 - 16weeks\n",
      "j.&amp;j --- j .  & amp ; j\n",
      "tk_tr --- tk _ tr\n",
      "al-#zanati --- al -  # zanati\n",
      "jab,time --- jab , time\n",
      "bio-pharma --- bio - pharma\n"
     ]
    }
   ],
   "source": [
    "# Try Split word\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 86438 | Known words: 16455\n",
      "yur1 --- yuri\n",
      "1bn --- ibn\n",
      "bl0nde --- blonde\n",
      "ph1 --- phi\n",
      "1ce --- ice\n",
      "sh1 --- shi\n",
      "b1tch --- bitch\n",
      "c0ck --- cock\n",
      "w0ng --- wong\n",
      "3ra --- era\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion \n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "            \n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    \n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = convert_leet(word)\n",
    "    if (new_word!=word): \n",
    "        if (len(word)>2) and (new_word in local_vocab):\n",
    "            temp_dict[word] = new_word\n",
    "    \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 78325 | Known words: 16699\n"
     ]
    }
   ],
   "source": [
    "# Open Holded words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in tweets for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (not check_replace(k))]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "tweets = tweets.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "tweets = tweets.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(tweets, local_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 77291 | Known words: 16788\n",
      "cocktails --- cocktail\n",
      "laces --- lace\n",
      "muses --- muse\n",
      "wingers --- winger\n",
      "hugss --- hugs\n",
      "danielas --- daniela\n",
      "complements --- complement\n",
      "ghanaians --- ghanaian\n",
      "marts --- mart\n",
      "radars --- radar\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert emoji to text:\n",
      "Unknown words: 77291 | Known words: 16788\n"
     ]
    }
   ],
   "source": [
    "# Convert emoji to text\n",
    "# Local \n",
    "temp_vocab = check_vocab(tweets, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k in emoji.UNICODE_EMOJI)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.compile('[:_]').sub(' ', emoji.UNICODE_EMOJI.get(word)) \n",
    "tweets = tweets.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert emoji to text:'); check_vocab(tweets, local_vocab);\n",
    "if verbose: print_dict(temp_dict)                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    same folks said daikon paste could treat a cyt...\n",
       "1    while the world has been on the wrong side of ...\n",
       "2    # coronavirus # sputnikv # astrazeneca # pfize...\n",
       "3    facts are immutable , senator , even when you ...\n",
       "4    explain to me again why we need a vaccine @ bo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>Rachel Roh</td>\n",
       "      <td>La Crescenta-Montrose, CA</td>\n",
       "      <td>Aggregator of Asian American news; scanning di...</td>\n",
       "      <td>2009-04-08 17:52:46</td>\n",
       "      <td>405</td>\n",
       "      <td>1692</td>\n",
       "      <td>3247</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>Albert Fong</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Marketing dude, tech geek, heavy metal &amp; '80s ...</td>\n",
       "      <td>2009-09-21 15:27:30</td>\n",
       "      <td>834</td>\n",
       "      <td>666</td>\n",
       "      <td>178</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>eliðŸ‡±ðŸ‡¹ðŸ‡ªðŸ‡ºðŸ‘Œ</td>\n",
       "      <td>Your Bed</td>\n",
       "      <td>heil, hydra ðŸ–â˜º</td>\n",
       "      <td>2020-06-25 23:30:28</td>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "      <td>155</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td># coronavirus # sputnikv # astrazeneca # pfize...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>Charles Adler</td>\n",
       "      <td>Vancouver, BC - Canada</td>\n",
       "      <td>Hosting \"CharlesAdlerTonight\" Global News Radi...</td>\n",
       "      <td>2008-09-10 11:28:53</td>\n",
       "      <td>49165</td>\n",
       "      <td>3933</td>\n",
       "      <td>21853</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>facts are immutable , senator , even when you ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>446</td>\n",
       "      <td>2129</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>Citizen News Channel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Citizen News Channel bringing you an alternati...</td>\n",
       "      <td>2020-04-23 17:58:42</td>\n",
       "      <td>152</td>\n",
       "      <td>580</td>\n",
       "      <td>1473</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>explain to me again why we need a vaccine @ bo...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1337852648389832708</td>\n",
       "      <td>Dee</td>\n",
       "      <td>Birmingham, England</td>\n",
       "      <td>Gastroenterology trainee, Clinical Research Fe...</td>\n",
       "      <td>2020-01-26 21:43:12</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>106</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:11:42</td>\n",
       "      <td>does anyone have any useful advice / guidance ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1337851215875608579</td>\n",
       "      <td>Gunther Fehlinger</td>\n",
       "      <td>Austria, Ukraine and Kosovo</td>\n",
       "      <td>End North Stream 2 now - the pipeline of corru...</td>\n",
       "      <td>2013-06-10 17:49:22</td>\n",
       "      <td>2731</td>\n",
       "      <td>5001</td>\n",
       "      <td>69344</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:06:00</td>\n",
       "      <td>it is a bit sad to claim the fame for success ...</td>\n",
       "      <td>['vaccination']</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1337850832256176136</td>\n",
       "      <td>Dr.Krutika Kuppalli</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ID, Global Health, VHF, Pandemic Prep, Emergin...</td>\n",
       "      <td>2019-03-25 04:14:29</td>\n",
       "      <td>21924</td>\n",
       "      <td>593</td>\n",
       "      <td>7815</td>\n",
       "      <td>True</td>\n",
       "      <td>2020-12-12 20:04:29</td>\n",
       "      <td>there have not been many bright days in 2020 b...</td>\n",
       "      <td>['BidenHarris', 'Election2020']</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1337850023531347969</td>\n",
       "      <td>Erin Despas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Designing&amp;selling on Teespring. Like 90s Disne...</td>\n",
       "      <td>2009-10-30 17:53:54</td>\n",
       "      <td>887</td>\n",
       "      <td>1515</td>\n",
       "      <td>9639</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 20:01:16</td>\n",
       "      <td>covid vaccine ; you getting it ? # covidvaccin...</td>\n",
       "      <td>['CovidVaccine', 'covid19', 'PfizerBioNTech', ...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1337842295857623042</td>\n",
       "      <td>Ch.Amjad Ali</td>\n",
       "      <td>Islamabad</td>\n",
       "      <td>#ProudPakistani #LovePakArmy #PMIK @insafiansp...</td>\n",
       "      <td>2012-11-12 04:18:12</td>\n",
       "      <td>671</td>\n",
       "      <td>2368</td>\n",
       "      <td>20469</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-12-12 19:30:33</td>\n",
       "      <td># covidvaccine states will start getting # cov...</td>\n",
       "      <td>['CovidVaccine', 'COVID19Vaccine', 'US', 'paku...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id             user_name                user_location  \\\n",
       "0  1340539111971516416            Rachel Roh    La Crescenta-Montrose, CA   \n",
       "1  1338158543359250433           Albert Fong            San Francisco, CA   \n",
       "2  1337858199140118533              eliðŸ‡±ðŸ‡¹ðŸ‡ªðŸ‡ºðŸ‘Œ                     Your Bed   \n",
       "3  1337855739918835717         Charles Adler       Vancouver, BC - Canada   \n",
       "4  1337854064604966912  Citizen News Channel                          NaN   \n",
       "5  1337852648389832708                   Dee          Birmingham, England   \n",
       "6  1337851215875608579     Gunther Fehlinger  Austria, Ukraine and Kosovo   \n",
       "7  1337850832256176136   Dr.Krutika Kuppalli                          NaN   \n",
       "8  1337850023531347969           Erin Despas                          NaN   \n",
       "9  1337842295857623042          Ch.Amjad Ali                    Islamabad   \n",
       "\n",
       "                                    user_description         user_created  \\\n",
       "0  Aggregator of Asian American news; scanning di...  2009-04-08 17:52:46   \n",
       "1  Marketing dude, tech geek, heavy metal & '80s ...  2009-09-21 15:27:30   \n",
       "2                                     heil, hydra ðŸ–â˜º  2020-06-25 23:30:28   \n",
       "3  Hosting \"CharlesAdlerTonight\" Global News Radi...  2008-09-10 11:28:53   \n",
       "4  Citizen News Channel bringing you an alternati...  2020-04-23 17:58:42   \n",
       "5  Gastroenterology trainee, Clinical Research Fe...  2020-01-26 21:43:12   \n",
       "6  End North Stream 2 now - the pipeline of corru...  2013-06-10 17:49:22   \n",
       "7  ID, Global Health, VHF, Pandemic Prep, Emergin...  2019-03-25 04:14:29   \n",
       "8  Designing&selling on Teespring. Like 90s Disne...  2009-10-30 17:53:54   \n",
       "9  #ProudPakistani #LovePakArmy #PMIK @insafiansp...  2012-11-12 04:18:12   \n",
       "\n",
       "   user_followers  user_friends  user_favourites  user_verified  \\\n",
       "0             405          1692             3247          False   \n",
       "1             834           666              178          False   \n",
       "2              10            88              155          False   \n",
       "3           49165          3933            21853           True   \n",
       "4             152           580             1473          False   \n",
       "5             105           108              106          False   \n",
       "6            2731          5001            69344          False   \n",
       "7           21924           593             7815           True   \n",
       "8             887          1515             9639          False   \n",
       "9             671          2368            20469          False   \n",
       "\n",
       "                  date                                               text  \\\n",
       "0  2020-12-20 06:06:44  same folks said daikon paste could treat a cyt...   \n",
       "1  2020-12-13 16:27:13  while the world has been on the wrong side of ...   \n",
       "2  2020-12-12 20:33:45  # coronavirus # sputnikv # astrazeneca # pfize...   \n",
       "3  2020-12-12 20:23:59  facts are immutable , senator , even when you ...   \n",
       "4  2020-12-12 20:17:19  explain to me again why we need a vaccine @ bo...   \n",
       "5  2020-12-12 20:11:42  does anyone have any useful advice / guidance ...   \n",
       "6  2020-12-12 20:06:00  it is a bit sad to claim the fame for success ...   \n",
       "7  2020-12-12 20:04:29  there have not been many bright days in 2020 b...   \n",
       "8  2020-12-12 20:01:16  covid vaccine ; you getting it ? # covidvaccin...   \n",
       "9  2020-12-12 19:30:33  # covidvaccine states will start getting # cov...   \n",
       "\n",
       "                                            hashtags               source  \\\n",
       "0                                 ['PfizerBioNTech']  Twitter for Android   \n",
       "1                                                NaN      Twitter Web App   \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  Twitter for Android   \n",
       "3                                                NaN      Twitter Web App   \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']   Twitter for iPhone   \n",
       "5                                                NaN   Twitter for iPhone   \n",
       "6                                    ['vaccination']      Twitter Web App   \n",
       "7                    ['BidenHarris', 'Election2020']   Twitter for iPhone   \n",
       "8  ['CovidVaccine', 'covid19', 'PfizerBioNTech', ...      Twitter Web App   \n",
       "9  ['CovidVaccine', 'COVID19Vaccine', 'US', 'paku...      Twitter Web App   \n",
       "\n",
       "   retweets  favorites  is_retweet  \n",
       "0         0          0       False  \n",
       "1         1          1       False  \n",
       "2         0          0       False  \n",
       "3       446       2129       False  \n",
       "4         0          0       False  \n",
       "5         0          0       False  \n",
       "6         0          4       False  \n",
       "7         2         22       False  \n",
       "8         2          1       False  \n",
       "9         0          0       False  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEETS['text'] = tweets\n",
    "TWEETS.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetSentiment(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.label\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.text[index]\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(text, None, add_special_tokens=True, max_length=self.max_len, pad_to_max_length=True, return_token_type_ids=True)\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids=inputs['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-04\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = LABELED.rename({'tweet_text': 'text'}, axis=1)\n",
    "labeled_tweets['label'] = labeled_tweets['label']-1\n",
    "labeled_tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "train_data=labeled_tweets.sample(frac=train_size,random_state=200)\n",
    "test_data=labeled_tweets.drop(train_data.index).reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(labeled_tweets.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = TweetSentiment(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = TweetSentiment(test_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RobertaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier0 = torch.nn.Linear(768, 768)\n",
    "        self.dropout0 = torch.nn.Dropout(0.3)\n",
    "        self.pre_classifier1 = torch.nn.Linear(768, 384)\n",
    "        self.dropout1 = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(384, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier0(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout0(pooler)\n",
    "        pooler = self.pre_classifier1(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout1(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model = RobertaClass()\n",
    "params = model.state_dict()\n",
    "params.keys()\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and 'l1' in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    acc = valid(model, testing_loader)\n",
    "    print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340539111971516416</td>\n",
       "      <td>405</td>\n",
       "      <td>2020-12-20 06:06:44</td>\n",
       "      <td>same folks said daikon paste could treat a cyt...</td>\n",
       "      <td>['PfizerBioNTech']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1338158543359250433</td>\n",
       "      <td>834</td>\n",
       "      <td>2020-12-13 16:27:13</td>\n",
       "      <td>while the world has been on the wrong side of ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1337858199140118533</td>\n",
       "      <td>10</td>\n",
       "      <td>2020-12-12 20:33:45</td>\n",
       "      <td># coronavirus # sputnikv # astrazeneca # pfize...</td>\n",
       "      <td>['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1337855739918835717</td>\n",
       "      <td>49165</td>\n",
       "      <td>2020-12-12 20:23:59</td>\n",
       "      <td>facts are immutable , senator , even when you ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1337854064604966912</td>\n",
       "      <td>152</td>\n",
       "      <td>2020-12-12 20:17:19</td>\n",
       "      <td>explain to me again why we need a vaccine @ bo...</td>\n",
       "      <td>['whereareallthesickpeople', 'PfizerBioNTech']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  user_followers                 date  \\\n",
       "0  1340539111971516416             405  2020-12-20 06:06:44   \n",
       "1  1338158543359250433             834  2020-12-13 16:27:13   \n",
       "2  1337858199140118533              10  2020-12-12 20:33:45   \n",
       "3  1337855739918835717           49165  2020-12-12 20:23:59   \n",
       "4  1337854064604966912             152  2020-12-12 20:17:19   \n",
       "\n",
       "                                                text  \\\n",
       "0  same folks said daikon paste could treat a cyt...   \n",
       "1  while the world has been on the wrong side of ...   \n",
       "2  # coronavirus # sputnikv # astrazeneca # pfize...   \n",
       "3  facts are immutable , senator , even when you ...   \n",
       "4  explain to me again why we need a vaccine @ bo...   \n",
       "\n",
       "                                            hashtags  \n",
       "0                                 ['PfizerBioNTech']  \n",
       "1                                                NaN  \n",
       "2  ['coronavirus', 'SputnikV', 'AstraZeneca', 'Pf...  \n",
       "3                                                NaN  \n",
       "4     ['whereareallthesickpeople', 'PfizerBioNTech']  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TWEETS.head(5)\n",
    "TWEETS = TWEETS.drop(columns=['user_name', 'user_location', 'user_description', 'user_created', 'user_friends', 'user_favourites', 'user_verified', 'source', 'retweets', 'favorites', 'is_retweet'])\n",
    "TWEETS.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS.to_csv('all_tweets_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting_tweets = TWEETS\n",
    "predicting_tweets.head(5)\n",
    "predicting_set = TweetSentiment(predicting_tweets, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Good night\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "model.to('cpu')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "\n",
    "# text = \"Good night ðŸ˜Š\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "print(scores)\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicting_loader = DataLoader(predicting_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELED.to_csv('annotated_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83927f8280031deb3aa1314dc0bf4bd4fab6545592770c6e05f6ffa72755e8b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
